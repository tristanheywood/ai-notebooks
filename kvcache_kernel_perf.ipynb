{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `kernel_perf.ipynb`, but after implementing KV-caching, so that only a single row of the attention matrix must be computed per iteration.\n",
    "\n",
    "Test the performance of various Triton kernels, varying the configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "import jax_triton as jt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt import get_config_for, param_shapes\n",
    "from nimblegpt.params import get_flaxmodels_gpt2_params, make_gpt_param_dict\n",
    "from nimblegpt.model import SingleHeadCausalSelfAttention\n",
    "from nimblegpt.jmodel import JSingleHeadCausalSelfAttention\n",
    "from nimblegpt.fast_model import FSingleHeadCausalSelfAttention, FGPT\n",
    "\n",
    "from nimblegpt.kernels.kvcache_triton_kernels import SHCSABlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for(\"gpt2\")\n",
    "n_cntx = config.block_size\n",
    "n_feat = config.n_embd // config.n_head\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_params = make_gpt_param_dict(get_flaxmodels_gpt2_params(), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(rng, (config.n_embd, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = FSingleHeadCausalSelfAttention(n_feat, n_cntx)\n",
    "params = module.init(rng, x, jnp.array(0))\n",
    "_, vars = module.apply(\n",
    "    params,\n",
    "    x,\n",
    "    jnp.array(0),\n",
    "    mutable=\"cache\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "jit_fshcsa = jax.jit(partial(FSingleHeadCausalSelfAttention(n_feat, n_cntx).apply, mutable=\"cache\"))\n",
    "jit_fshcsa({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0]\n",
    "\n",
    "%timeit -n100 jit_fshcsa({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 µs ± 15.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "jit_block = jax.jit(partial(SHCSABlock(n_feat, n_cntx).apply, mutable=\"cache\"))\n",
    "jit_block({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0]\n",
    "\n",
    "%timeit -n100 jit_block({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691 µs ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "jit_block = jax.jit(partial(SHCSABlock(n_feat, n_cntx, subseq_size = 128, subfeat_size=16).apply, mutable=\"cache\"))\n",
    "jit_block({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0]\n",
    "\n",
    "%timeit -n100 jit_block({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0].block_until_ready()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
