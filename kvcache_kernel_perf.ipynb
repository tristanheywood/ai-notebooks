{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `kernel_perf.ipynb`, but after implementing KV-caching, so that only a single row of the attention matrix must be computed per iteration.\n",
    "\n",
    "Test the performance of various Triton kernels, varying the configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "import jax_triton as jt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt import get_config_for, param_shapes\n",
    "from nimblegpt.params import get_flaxmodels_gpt2_params, make_gpt_param_dict\n",
    "from nimblegpt.model import SingleHeadCausalSelfAttention\n",
    "from nimblegpt.jmodel import JSingleHeadCausalSelfAttention\n",
    "from nimblegpt.fast_model import FSingleHeadCausalSelfAttention, FGPT\n",
    "\n",
    "from nimblegpt.kernels.kvcache_triton_kernels import SHCSABlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for(\"gpt2\")\n",
    "n_cntx = config.block_size\n",
    "n_feat = config.n_embd // config.n_head\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_params = make_gpt_param_dict(get_flaxmodels_gpt2_params(), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(rng, (config.n_embd, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = FSingleHeadCausalSelfAttention(n_feat, n_cntx)\n",
    "params = module.init(rng, x, jnp.array(0))\n",
    "_, vars = module.apply(\n",
    "    params,\n",
    "    x,\n",
    "    jnp.array(0),\n",
    "    mutable=\"cache\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733 µs ± 44.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "jit_fshcsa = jax.jit(partial(FSingleHeadCausalSelfAttention(n_feat, n_cntx).apply, mutable=\"cache\"))\n",
    "jit_fshcsa({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0]\n",
    "\n",
    "%timeit -n100 jit_fshcsa({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 µs ± 22.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "jit_block = jax.jit(partial(SHCSABlock(n_feat, n_cntx).apply, mutable=\"cache\"))\n",
    "jit_block({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0]\n",
    "\n",
    "%timeit -n100 jit_block({**params, \"cache\": vars[\"cache\"]}, x, jnp.array(0))[0].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_idxs = jax.random.randint(rng, (3, ), 0, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodule = FGPT.Make(config)\n",
    "vars = fmodule.init_vars({\"params\": gpt_params})\n",
    "\n",
    "fseq = fmodule.generate(\n",
    "    rng,\n",
    "    {\n",
    "        \"params\": gpt_params,\n",
    "        **vars\n",
    "    },\n",
    "    prompt_idxs,\n",
    "    max_new_tokens=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.6 ms ± 716 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n10\n",
    "fmodule.generate(\n",
    "    rng,\n",
    "    {\n",
    "        \"params\": gpt_params,\n",
    "        **vars\n",
    "    },\n",
    "    prompt_idxs,\n",
    "    max_new_tokens=10,\n",
    ").block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Batching rule for 'triton_kernel_call' not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m block_module \u001b[39m=\u001b[39m FGPT\u001b[39m.\u001b[39mMakeWithSHCSA(config, partial(SHCSABlock, subseq_size \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[39mvars\u001b[39m \u001b[39m=\u001b[39m block_module\u001b[39m.\u001b[39;49minit_vars({\u001b[39m\"\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m\"\u001b[39;49m: gpt_params})\n\u001b[1;32m      4\u001b[0m fseq \u001b[39m=\u001b[39m block_module\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m      5\u001b[0m     rng,\n\u001b[1;32m      6\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/flax/flax/linen/module.py:423\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    422\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 423\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/flax/flax/linen/module.py:820\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    819\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m--> 820\u001b[0m     y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/nimbleGPT/nimblegpt/fast_model.py:210\u001b[0m, in \u001b[0;36mFGPT.init_vars\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_vars\u001b[39m(\u001b[39mself\u001b[39m, params) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[1;32m    201\u001b[0m     \u001b[39m\"\"\"Initializes (and returns) the model's variables (i.e. the KV cache).\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m    Example usage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     _, \u001b[39mvars\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(params,\n\u001b[1;32m    211\u001b[0m                          jnp\u001b[39m.\u001b[39;49marray(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m    212\u001b[0m                          jnp\u001b[39m.\u001b[39;49marray(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m    213\u001b[0m                          mutable\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcache\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mvars\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/nimbleGPT/nimblegpt/fast_model.py:189\u001b[0m, in \u001b[0;36mFGPT.__call__\u001b[0;34m(self, tok_idx, seq_idx)\u001b[0m\n\u001b[1;32m    186\u001b[0m x \u001b[39m=\u001b[39m tok_emb \u001b[39m+\u001b[39m pos_emb\n\u001b[1;32m    188\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC\u001b[39m.\u001b[39mn_layer):\n\u001b[0;32m--> 189\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBlock(n_cntx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC\u001b[39m.\u001b[39;49mblock_size,\n\u001b[1;32m    190\u001b[0m                    n_head\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC\u001b[39m.\u001b[39;49mn_head)(x, seq_idx)\n\u001b[1;32m    192\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm()(x)\n\u001b[1;32m    193\u001b[0m logits \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDense(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC\u001b[39m.\u001b[39mvocab_size, use_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/nimbleGPT/nimblegpt/fast_model.py:138\u001b[0m, in \u001b[0;36mFBlock.__call__\u001b[0;34m(self, x, seq_idx)\u001b[0m\n\u001b[1;32m    135\u001b[0m C, \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape  \u001b[39m# Embedding dimensionality.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m y \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm()(x)\n\u001b[0;32m--> 138\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mCausalSelfAttention(n_cntx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_cntx,\n\u001b[1;32m    139\u001b[0m                              n_head\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_head)(y, seq_idx)\n\u001b[1;32m    140\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m y\n\u001b[1;32m    142\u001b[0m y \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm()(x)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/nimbleGPT/nimblegpt/fast_model.py:110\u001b[0m, in \u001b[0;36mFCausalSelfAttention.__call__\u001b[0;34m(self, x, seq_idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m n_feat \u001b[39m=\u001b[39m C \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_head  \u001b[39m# Features per q/k/v per head.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# [C,] -> [n_head, n_feat]\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m y \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mvmap(\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSingleHeadCausalSelfAttention,\n\u001b[1;32m    112\u001b[0m     in_axes\u001b[39m=\u001b[39;49m\n\u001b[1;32m    113\u001b[0m     \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Don't map over `x` - each `SingleHead CausalSelfAttention` gets the full `x`.\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m     axis_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_head,\n\u001b[1;32m    115\u001b[0m     out_axes\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    116\u001b[0m     variable_axes\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    117\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m,\n\u001b[1;32m    118\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mcache\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m\n\u001b[1;32m    119\u001b[0m     },  \u001b[39m# 0th axis of params should be the vmap axis.\u001b[39;49;00m\n\u001b[1;32m    120\u001b[0m     split_rngs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m},\n\u001b[1;32m    121\u001b[0m )(n_feat\u001b[39m=\u001b[39;49mn_feat, n_cntx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_cntx)(x, seq_idx)\n\u001b[1;32m    122\u001b[0m y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mreshape(y, (C, ))  \u001b[39m# [n_head, n_feat] -> [C,]\u001b[39;00m\n\u001b[1;32m    124\u001b[0m y \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDense(features\u001b[39m=\u001b[39mC)(y)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/nimbleGPT/nimblegpt/kernels/kvcache_triton_kernels.py:193\u001b[0m, in \u001b[0;36mSHCSABlock.__call__\u001b[0;34m(self, x, seq_idx)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x: jax\u001b[39m.\u001b[39mArray, seq_idx: jax\u001b[39m.\u001b[39mArray):\n\u001b[1;32m    192\u001b[0m     q, K, V \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_qKV(x, seq_idx)\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m shcsa_block(q,\n\u001b[1;32m    194\u001b[0m                        K,\n\u001b[1;32m    195\u001b[0m                        V,\n\u001b[1;32m    196\u001b[0m                        seq_idx,\n\u001b[1;32m    197\u001b[0m                        SUBSEQ_SIZE\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubseq_size,\n\u001b[1;32m    198\u001b[0m                        SUBFEAT_SIZE\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubfeat_size)\n",
      "File \u001b[0;32m~/nimbleGPT/nimblegpt/kernels/kvcache_triton_kernels.py:172\u001b[0m, in \u001b[0;36mshcsa_block\u001b[0;34m(q, K, V, seq_idx, SUBSEQ_SIZE, SUBFEAT_SIZE)\u001b[0m\n\u001b[1;32m    169\u001b[0m out_shape \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mShapeDtypeStruct((N_FEAT, ), q\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    170\u001b[0m grid \u001b[39m=\u001b[39m (N_FEAT \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m SUBFEAT_SIZE, )\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m jt\u001b[39m.\u001b[39;49mtriton_call(q,\n\u001b[1;32m    173\u001b[0m                       K,\n\u001b[1;32m    174\u001b[0m                       V,\n\u001b[1;32m    175\u001b[0m                       seq_idx,\n\u001b[1;32m    176\u001b[0m                       kernel\u001b[39m=\u001b[39;49mshcsa_block_kernel,\n\u001b[1;32m    177\u001b[0m                       out_shape\u001b[39m=\u001b[39;49mout_shape,\n\u001b[1;32m    178\u001b[0m                       grid\u001b[39m=\u001b[39;49mgrid,\n\u001b[1;32m    179\u001b[0m                       SM_SCALE\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m N_FEAT\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m    180\u001b[0m                       SEQ_LEN\u001b[39m=\u001b[39;49mK\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m                       N_FEAT\u001b[39m=\u001b[39;49mN_FEAT,\n\u001b[1;32m    182\u001b[0m                       SUBSEQ_SIZE\u001b[39m=\u001b[39;49mSUBSEQ_SIZE,\n\u001b[1;32m    183\u001b[0m                       SUBFEAT_SIZE\u001b[39m=\u001b[39;49mSUBFEAT_SIZE)\n",
      "File \u001b[0;32m~/jax-triton/jax_triton/triton_call.py:277\u001b[0m, in \u001b[0;36mtriton_call\u001b[0;34m(kernel, out_shape, grid, call_name, num_warps, num_stages, dump_binary_path, input_output_aliases, debug, *args, **metaparams)\u001b[0m\n\u001b[1;32m    273\u001b[0m kernel_name, asm_map, shared_mem \u001b[39m=\u001b[39m compile_triton_func(\n\u001b[1;32m    274\u001b[0m   avals_in, avals_out, kernel, num_warps, num_stages, metaparams,\n\u001b[1;32m    275\u001b[0m   dump\u001b[39m=\u001b[39mdebug)\n\u001b[1;32m    276\u001b[0m asm \u001b[39m=\u001b[39m Asm(asm_map)\n\u001b[0;32m--> 277\u001b[0m out_flat \u001b[39m=\u001b[39m triton_kernel_call_p\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49mflat_args, kernel_name\u001b[39m=\u001b[39;49mkernel_name,\n\u001b[1;32m    278\u001b[0m     call_name\u001b[39m=\u001b[39;49mcall_name, asm\u001b[39m=\u001b[39;49masm,\n\u001b[1;32m    279\u001b[0m     shared_mem\u001b[39m=\u001b[39;49mshared_mem, out_shapes\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(flat_out_shapes),\n\u001b[1;32m    280\u001b[0m     grid\u001b[39m=\u001b[39;49mgrid, num_warps\u001b[39m=\u001b[39;49mnum_warps, num_stages\u001b[39m=\u001b[39;49mnum_stages,\n\u001b[1;32m    281\u001b[0m     dump_binary_path\u001b[39m=\u001b[39;49mdump_binary_path,\n\u001b[1;32m    282\u001b[0m    input_output_aliases\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(input_output_aliases\u001b[39m.\u001b[39;49mitems()), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmetaparams)\n\u001b[1;32m    283\u001b[0m \u001b[39mreturn\u001b[39;00m tree_util\u001b[39m.\u001b[39mtree_unflatten(out_tree, out_flat)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jax/interpreters/batching.py:327\u001b[0m, in \u001b[0;36mBatchTrace.get_primitive_batcher\u001b[0;34m(self, primitive, frame)\u001b[0m\n\u001b[1;32m    325\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_axis_primitive_batcher(primitive, frame)\n\u001b[1;32m    326\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBatching rule for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not implemented\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 327\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(primitive))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Batching rule for 'triton_kernel_call' not implemented"
     ]
    }
   ],
   "source": [
    "block_module = FGPT.MakeWithSHCSA(config, partial(SHCSABlock, subseq_size = 128))\n",
    "vars = block_module.init_vars({\"params\": gpt_params})\n",
    "\n",
    "fseq = block_module.generate(\n",
    "    rng,\n",
    "    {\n",
    "        \"params\": gpt_params,\n",
    "        **vars\n",
    "    },\n",
    "    prompt_idxs,\n",
    "    max_new_tokens=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
