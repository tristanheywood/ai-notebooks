{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed up fast_model by implementing single headed self attention using a triton kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "import jax_triton as jt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt import get_config_for\n",
    "from nimblegpt.fast_model import FSingleHeadCausalSelfAttention, FGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for('gpt2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Trick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block kernel, we accumulate the final output incrementally in pieces. When a new piece is computed, we must 'undo' and 'redo' the softmax on the current accumulation, so as to use the new maximum and normalization factor. Suppose we have two attention vector pieces $\\bm{a}^{(1)}$ and $\\bm{a}^{(2)}$ and two corresponding blocks of the value matrix $\\bm{V}_1$ and $\\bm{V}_2$. Let $\\bm{v}^{(1)}$ and $\\bm{v}^{(2)}$ denote the first columns of $\\bm{V}_1$ and $\\bm{V}_2$ respectively. By focusing on the first column, we examine a single element of the output vector - but the result is generalizable to the entire output vector.\n",
    "\n",
    "Define $m^{(1)}$ and $m^{(2)}$ to be the maximum of $\\bm{a}^{(1)}$ and $\\bm{a}^{(2)}$ respectively, and similarly $\\ell^{(1)}$ and $\\ell^{(2)}$ to be the normalization factors.\n",
    "\n",
    "Suppose we have already computed the softmax-dot-V for each block:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y_1 &= \\frac{e^{\\bm{a}^{(1)} - m^{(1)}}}{\\ell_1} \\cdot \\bm{v}^{(1)}  = \\frac{1}{\\ell^{(1)}} \\sum e^{a^{(1)}_i - m^{(1)}} \\cdot v^{(1)}_i\\\\\n",
    "    y_2 &= \\frac{e^{\\bm{a}^{(2)} - m^{(2)}}}{\\ell_2} \\cdot \\bm{v}^{(2)} = \\frac{1}{\\ell^{(2)}} \\sum e^{a^{(2)}_i - m^{(2)}} \\cdot v^{(2)}_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let $m$ and $\\ell$ denote our new maximum and normalization factor:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    m &= \\max(m^{(1)}, m^{(2)})\\\\\n",
    "    \\ell &= e^{m^{(1)} - m} \\ell^{(1)} + e^{m^{(2)} - m} \\ell^{(2)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that this follows since:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    e^{m^{(1)} - m} \\ell^{(1)} + e^{m^{(2)} - m} \\ell^{(2)} &= e^{m^{(1)} -m} \\sum e^{a^{(1)}_i - m^{(1)}} + e^{m^{(2)} - m} \\sum e^{a^{(2)}_i - m^{(2)}} \\\\\n",
    "    &= \\sum e^{a^{(1)}_i - m} + \\sum e^{a^{(2)}_i - m} \\\\\n",
    "    &= \\ell\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We wish to combine $y_1$ and $y_2$ into an accumulated output, with the new maximum and normalization factor. We know that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y &= \\frac{e^{\\bm{a}^{(1)} - m}}{\\ell} \\cdot \\bm{v}^{(1)} + \\frac{e^{\\bm{a}^{(2)} - m}}{\\ell} \\cdot \\bm{v}^{(2)} \\\\\n",
    "    &= \\frac{\\ell^{(1)}}{\\ell^{(1)}} \\frac{e^{m - m^{(1)}}}{e^{m - m^{(1)}}} \\frac{e^{\\bm{a}^{(1)} - m}}{\\ell} \\cdot \\bm{v}^{(1)} + \\frac{\\ell^{(2)}}{\\ell^{(2)}} \\frac{e^{m - m^{(2)}}}{e^{m - m^{(2)}}} \\frac{e^{\\bm{a}^{(2)} - m}}{\\ell} \\cdot \\bm{v}^{(2)} \\\\\n",
    "    &= \\frac{\\ell^{(1)}}{\\ell \\cdot e^{m - m^{(1)}}} \\frac{e^{\\bm{a}^{(1)} - m^{(1)}}}{\\ell^{(1)}} \\cdot \\bm{v}^{(1)} + \\frac{\\ell^{(2)}}{\\ell \\cdot e^{m - m^{(2)}}} \\frac{e^{\\bm{a}^{(2)} - m^{(2)}}}{\\ell^{(2)}} \\cdot \\bm{v}^{(2)} \\\\\n",
    "    &= \\frac{\\ell^{(1)}}{\\ell \\cdot e^{m - m^{(1)}}} y_1 + \\frac{\\ell^{(2)}}{\\ell \\cdot e^{m - m^{(2)}}} y_2 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def resoftmax(y1, y2, m1, m2, l1, l2):\n",
    "\n",
    "    m = tl.where(m1 > m2, m1, m2)\n",
    "    l = tl.exp(m1 - m) * l1 + tl.exp(m2 - m) * l2\n",
    "\n",
    "    return (l1 / tl.exp(m - m1) * y1 + l2 / tl.exp(m - m2) * y2) / l, m, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def shcsa_block_kernel(q_ptr, K_ptr, V_ptr, seq_idx_ptr, out_ptr,\n",
    "                       SM_SCALE: tl.constexpr, SEQ_LEN: tl.constexpr,\n",
    "                       N_FEAT: tl.constexpr, SUBSEQ_SIZE: tl.constexpr,\n",
    "                       SUBFEAT_SIZE: tl.constexpr):\n",
    "    \"\"\"\n",
    "    Triton kernel implementing single-headed attention with causal masking for a single\n",
    "    token embedding. The kernel computes `SEBSEQ_SIZE` chunks of the attention vector, \n",
    "    flash-attention style. Each kernel cell computes `SUBFEAT_SIZE` elements of the output \n",
    "    vector.\n",
    "\n",
    "    For clarity, we call subsets of the sequence axis 'subseqs' and subsets of the feature\n",
    "    axis 'subfeats'. We call a tensor a 'block' when it has shape [SUBSEQ_SIZE,] or\n",
    "    [SUBSEQ_SIZE, N_FEAT], and a 'chunk' when it has size [SUBFEAT_SIZE,] or \n",
    "    [SUBSEQ_SIZE, SUBFEAT_SIZE].\n",
    "\n",
    "    Kernel cell i comuputes out[i * CHUNK_SIZE, (i+1)* CHUNK_SIZE], by multiplying `att` \n",
    "    with v[:, i * CHUNK_SIZE, (i+1)* CHUNK_SIZE] and summing over the sequence dimension.\n",
    "\n",
    "    As with flash-attention, the output is computed interatively in blocks. The sequence\n",
    "    is split into `SEQ_LEN // SUBSEQ_SIZE` blocks of size `SUBSEQ_SIZE`. \n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    q_ptr: [N_FEAT] - query vector for the current token.\n",
    "    K_ptr: [SEQ_LEN, N_FEAT] - key matrix for the entire sequence.\n",
    "    V_ptr: [SEQ_LEN, N_FEAT] - value matrix for the entire sequence.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    out_ptr: [N_FEAT] - self attention output (`att @ v`)\n",
    "    \"\"\"\n",
    "    seq_idx = tl.load(seq_idx_ptr)\n",
    "\n",
    "    subfeat_start = tl.program_id(0) * SUBFEAT_SIZE\n",
    "    # This cell computes out[out_row_num, out_col_start: out_col_start + N_OCOLS].\n",
    "\n",
    "    seq_idxs = tl.arange(0, SEQ_LEN)\n",
    "    feat_idxs = tl.arange(0, N_FEAT)\n",
    "\n",
    "    subfeat_idxs = tl.arange(0, SUBFEAT_SIZE) + subfeat_start\n",
    "\n",
    "    q = tl.load(q_ptr + feat_idxs)  # [N_FEAT,]\n",
    "\n",
    "    y_chunk_acc = tl.zeros((SUBFEAT_SIZE, ), dtype=tl.float32)\n",
    "    ## Running softmax - flash-attention style.\n",
    "    # Running attention maximum.\n",
    "    m_acc = float(\"-inf\")\n",
    "    # Running softmax denoniator.\n",
    "    l_acc = 0.0\n",
    "\n",
    "    # Don't bother calculating attention and outputs for tokens which are masked.\n",
    "    n_subseq = tl.cdiv(seq_idx + 1, SUBSEQ_SIZE)\n",
    "\n",
    "    for subseq_i in range(0, n_subseq):\n",
    "        # Each iteration, we load a [SEBSEQ_SIZE, N_FEAT] block of `K`` and compute a\n",
    "        # [SUBSEQ_SIZE,] block of `att`. We mulitply this by a [SEBSEQ_SIZE, CHUNK_SIZE]\n",
    "        # block of `V`` to compute a [CHUNK_SIZE,] partial-result block of `out.`\n",
    "\n",
    "        # Index io tokens of the sequence which are processed in this block.\n",
    "        subseq_idxs = tl.arange(0, SUBSEQ_SIZE) + subseq_i\n",
    "        # Causal mask for sequence tokens in this block.\n",
    "        subseq_mask = subseq_idxs <= seq_idx\n",
    "\n",
    "        # Index and mask into K. Sizes [SUBSEQ_SIZE, N_FEAT].\n",
    "        block_idxs = subseq_idxs[:, None] * N_FEAT + feat_idxs[None, :]\n",
    "        block_mask = tl.broadcast_to(subseq_mask[:, None],\n",
    "                                     (SUBSEQ_SIZE, N_FEAT))\n",
    "\n",
    "        K_block = tl.load(K_ptr + block_idxs,\n",
    "                          mask=block_mask, other=0.0)  # [SUBSEQ_SIZE, N_FEAT]\n",
    "\n",
    "        att_block = tl.sum(q[None, :] * K_block,\n",
    "                           axis=1) * SM_SCALE  # [BLOCK_SIZE,]\n",
    "        catt_block = tl.where(subseq_mask, att_block,\n",
    "                              float(\"-inf\"))  # [BLOCK_SIZE,]\n",
    "\n",
    "        max_block = tl.max(catt_block, axis=0)\n",
    "        # Softmax numerator of this block.\n",
    "        sm_num_block = tl.exp(catt_block - max_block)\n",
    "        # Softmax denominator of this block.\n",
    "        sm_den_block = tl.sum(sm_num_block, axis=0)\n",
    "        sm_att_block = sm_num_block / sm_den_block\n",
    "\n",
    "        # Load V[(block_num: block_num+1) * BLOCK_SIZE, out_col_start: out_col_start + N_OCOLS]\n",
    "\n",
    "        # Index and mask into V. Sizes [SUBSEQ_SIZE, SUBFEAT_SIZE].\n",
    "        chunk_idxs = subseq_idxs[:, None] * N_FEAT + subfeat_idxs[None, :]\n",
    "        chunk_mask = tl.broadcast_to(subseq_mask[:, None],\n",
    "                                     (SUBSEQ_SIZE, SUBFEAT_SIZE))\n",
    "\n",
    "        V_chunk = tl.load(V_ptr + chunk_idxs,\n",
    "                          mask=chunk_mask)  # [SUBSEQ_SIZE, SUBFEAT_SIZE]\n",
    "\n",
    "        # Partial result of a chunk of the output.\n",
    "        # ([SUBSEQ_SIZE,] -> [SUBSEQ_SIZE, 1]) * [SUBSEQ_SIZE, SUBFEAT_SIZE]\n",
    "        # -> [SUBSEQ_SIZE, SUBFEAT_SIZE] -{sum}-> [SUBFEAT_SIZE,]\n",
    "        out_chunk_pr = tl.sum(sm_att_block[:, None] * V_chunk, axis=0)\n",
    "\n",
    "        y_chunk_acc, m_acc, l_acc = resoftmax(y_chunk_acc, out_chunk_pr, m_acc,\n",
    "                                              max_block, l_acc, sm_den_block)\n",
    "\n",
    "    tl.store(out_ptr + subfeat_idxs, y_chunk_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shcsa_block(q,\n",
    "                K,\n",
    "                V,\n",
    "                seq_idx,\n",
    "                SUBSEQ_SIZE: int = 128,\n",
    "                SUBFEAT_SIZE: int = 32):\n",
    "    N_FEAT = q.shape[0]\n",
    "\n",
    "    out_shape = jax.ShapeDtypeStruct((N_FEAT, ), q.dtype)\n",
    "    grid = (N_FEAT // SUBFEAT_SIZE, )\n",
    "\n",
    "    return jt.triton_call(q,\n",
    "                          K,\n",
    "                          V,\n",
    "                          seq_idx,\n",
    "                          kernel=shcsa_block_kernel,\n",
    "                          out_shape=out_shape,\n",
    "                          grid=grid,\n",
    "                          SM_SCALE=1.0 / N_FEAT**0.5,\n",
    "                          SEQ_LEN=K.shape[0],\n",
    "                          N_FEAT=N_FEAT,\n",
    "                          SUBSEQ_SIZE=SUBSEQ_SIZE,\n",
    "                          SUBFEAT_SIZE=SUBFEAT_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHCSABlock(FSingleHeadCausalSelfAttention):\n",
    "    subseq_size: int = 128\n",
    "    subfeat_size: int = 32\n",
    "\n",
    "    def __call__(self, x: jax.Array, seq_idx: jax.Array):\n",
    "\n",
    "        q, K, V = self.get_qKV(x, seq_idx)\n",
    "        return shcsa_block(q, K, V, seq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.2799144 , -0.39865986, -0.5993886 , -0.7637496 , -0.8983587 ],      dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "n_feat = config.n_embd // config.n_head\n",
    "n_cntx = config.block_size\n",
    "x = jax.random.normal(rng, (n_feat,))\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "fshcsa_module = FSingleHeadCausalSelfAttention(n_cntx=n_cntx, n_feat = n_feat)\n",
    "vars = fshcsa_module.init(rng, x, jnp.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.76740426,  0.45793283,  0.7712825 , -0.71566945, -1.3524578 ,\n",
       "       -0.02418369,  0.33236438, -0.27028525,  0.38507232,  0.7667557 ,\n",
       "        0.2532947 , -0.21232384, -1.6268221 , -0.52965856,  0.79835236,\n",
       "        1.1416652 , -0.0675956 , -0.43777275, -0.4003198 ,  1.12303   ,\n",
       "       -0.05400813,  0.42411083, -1.8518133 ,  1.2761084 , -1.313626  ,\n",
       "        0.08351888, -2.1435814 ,  1.9459411 , -0.9885833 ,  0.07802778,\n",
       "        0.9368881 ,  1.2056795 , -0.74474347,  0.74293506,  0.71277654,\n",
       "       -0.7897532 ,  0.46426433, -0.13851726, -1.3792179 ,  0.72289133,\n",
       "        0.39826477, -0.21821803,  0.9840089 , -0.5381244 ,  0.7619692 ,\n",
       "        0.31365275, -0.591218  ,  0.48691204,  0.8179485 , -0.8554354 ,\n",
       "        1.0123616 , -0.7641751 ,  0.3230685 ,  0.06167361,  1.16292   ,\n",
       "       -0.36336797, -0.5731621 ,  0.78331184,  0.821721  , -0.5655025 ,\n",
       "       -1.2870891 ,  0.7006638 ,  0.5295207 , -0.087327  ], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fy, fvars = fshcsa_module.apply(vars, x, jnp.array(0), mutable=\"cache\")\n",
    "fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.76740426,  0.45793283,  0.7712825 , -0.71566945, -1.3524578 ,\n",
       "       -0.02418369,  0.33236438, -0.27028525,  0.38507232,  0.7667557 ,\n",
       "        0.2532947 , -0.21232384, -1.6268221 , -0.52965856,  0.79835236,\n",
       "        1.1416652 , -0.0675956 , -0.43777275, -0.4003198 ,  1.12303   ,\n",
       "       -0.05400813,  0.42411083, -1.8518133 ,  1.2761084 , -1.313626  ,\n",
       "        0.08351888, -2.1435814 ,  1.9459411 , -0.9885833 ,  0.07802778,\n",
       "        0.9368881 ,  1.2056795 , -0.74474347,  0.74293506,  0.71277654,\n",
       "       -0.7897532 ,  0.46426433, -0.13851726, -1.3792179 ,  0.72289133,\n",
       "        0.39826477, -0.21821803,  0.9840089 , -0.5381244 ,  0.7619692 ,\n",
       "        0.31365275, -0.591218  ,  0.48691204,  0.8179485 , -0.8554354 ,\n",
       "        1.0123616 , -0.7641751 ,  0.3230685 ,  0.06167361,  1.16292   ,\n",
       "       -0.36336797, -0.5731621 ,  0.78331184,  0.821721  , -0.5655025 ,\n",
       "       -1.2870891 ,  0.7006638 ,  0.5295207 , -0.087327  ], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = SHCSABlock(n_cntx=n_cntx, n_feat = n_feat)\n",
    "y, _ = module.apply(vars, x, jnp.array(0), mutable=\"cache\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0., dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fy - y).max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def shcsa_row_kernel(\n",
    "    q_ptr,\n",
    "    K_ptr,\n",
    "    V_ptr,\n",
    "    seq_idx_ptr,\n",
    "    out_ptr,\n",
    "    SM_SCALE: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    N_FEAT: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel implementing single-headed attention with causal masking for a single\n",
    "    token embedding. The kernel computes the whole attention row in one go.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    q_ptr: [N_FEAT] - query vector for the current token.\n",
    "    K_ptr: [SEQ_LEN, N_FEAT] - key matrix for the entire sequence.\n",
    "    V_ptr: [SEQ_LEN, N_FEAT] - value matrix for the entire sequence.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    out_ptr: [N_FEAT] - self attention output (`att @ v`)\n",
    "    \"\"\"\n",
    "    seq_idx = tl.load(seq_idx_ptr)\n",
    "\n",
    "    seq_idxs = tl.arange(0, SEQ_LEN)\n",
    "    feat_idxs = tl.arange(0, N_FEAT)\n",
    "    mat_idxs = seq_idxs[:, None] * N_FEAT + feat_idxs[None, :]\n",
    "\n",
    "    # Shape (SEQ_LEN,) mask. 0 for all indices i > seq_idx.\n",
    "    causal_seq_mask = seq_idxs <= seq_idx\n",
    "    # Shape (SEQ_LEN, N_FEAT) mask. 0 for all rows where rows[i] > seq_idx.\n",
    "    causal_mat_mask = tl.broadcast_to(causal_seq_mask[:, None], (SEQ_LEN, N_FEAT))\n",
    "\n",
    "    q = tl.load(q_ptr + feat_idxs)\n",
    "    K = tl.load(K_ptr + mat_idxs, mask=causal_mat_mask, other=0.0)\n",
    "    V = tl.load(V_ptr + mat_idxs, mask=causal_mat_mask, other=0.0)\n",
    "\n",
    "    # ([N_FEAT,] -> [1, N_FEAT]) * [SEQ_LEN, N_FEAT] -> [SEQ_LEN, N_FEAT] -{sum}-> [SEQ_LEN,]\n",
    "    # att[i] is high when token `seq_idx` should attend heavily to token i.\n",
    "    att = tl.sum(q[None, :] * K, axis=1) * SM_SCALE\n",
    "\n",
    "    causal_att = tl.where(causal_seq_mask, att, float(\"-inf\"))\n",
    "\n",
    "    sm_numerator = tl.exp(causal_att - tl.max(causal_att, axis=0))\n",
    "    sm_att = sm_numerator / tl.sum(sm_numerator, axis=0) # [SEQ_LEN,]\n",
    "\n",
    "    # ([SEQ_LEN,] -> [SEQ_LEN, 1]) * [SEQ_LEN, N_FEAT] -> [SEQ_LEN, N_FEAT] -{sum}-> [N_FEAT,]\n",
    "    out = tl.sum(sm_att[:, None] * V, axis=0) # [N_FEAT,]\n",
    "\n",
    "    tl.store(out_ptr + feat_idxs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shcsa_row(q, K, V, seq_idx):\n",
    "    N_FEAT = q.shape[0]\n",
    "\n",
    "    out_shape = jax.ShapeDtypeStruct((N_FEAT,), q.dtype)\n",
    "    grid = (1,)\n",
    "\n",
    "    return jt.triton_call(\n",
    "        q, K, V, seq_idx, kernel=shcsa_row_kernel, out_shape=out_shape, grid=grid, SM_SCALE = 1.0 / N_FEAT ** 0.5, SEQ_LEN = K.shape[0], N_FEAT = N_FEAT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHCSARow(FSingleHeadCausalSelfAttention):\n",
    "\n",
    "    def __call__(self, x: jax.Array, seq_idx: jax.Array):\n",
    "\n",
    "        q, K, V = self.get_qKV(x, seq_idx)\n",
    "        return shcsa_row(q, K, V, seq_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.2799144 , -0.39865986, -0.5993886 , -0.7637496 , -0.8983587 ],      dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "n_feat = config.n_embd // config.n_head\n",
    "n_cntx = config.block_size\n",
    "x = jax.random.normal(rng, (n_feat,))\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "fshcsa_module = FSingleHeadCausalSelfAttention(n_cntx=n_cntx, n_feat = n_feat)\n",
    "vars = fshcsa_module.init(rng, x, jnp.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.76740426,  0.45793283,  0.7712825 , -0.71566945, -1.3524578 ,\n",
       "       -0.02418369,  0.33236438, -0.27028525,  0.38507232,  0.7667557 ,\n",
       "        0.2532947 , -0.21232384, -1.6268221 , -0.52965856,  0.79835236,\n",
       "        1.1416652 , -0.0675956 , -0.43777275, -0.4003198 ,  1.12303   ,\n",
       "       -0.05400813,  0.42411083, -1.8518133 ,  1.2761084 , -1.313626  ,\n",
       "        0.08351888, -2.1435814 ,  1.9459411 , -0.9885833 ,  0.07802778,\n",
       "        0.9368881 ,  1.2056795 , -0.74474347,  0.74293506,  0.71277654,\n",
       "       -0.7897532 ,  0.46426433, -0.13851726, -1.3792179 ,  0.72289133,\n",
       "        0.39826477, -0.21821803,  0.9840089 , -0.5381244 ,  0.7619692 ,\n",
       "        0.31365275, -0.591218  ,  0.48691204,  0.8179485 , -0.8554354 ,\n",
       "        1.0123616 , -0.7641751 ,  0.3230685 ,  0.06167361,  1.16292   ,\n",
       "       -0.36336797, -0.5731621 ,  0.78331184,  0.821721  , -0.5655025 ,\n",
       "       -1.2870891 ,  0.7006638 ,  0.5295207 , -0.087327  ], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fy, fvars = fshcsa_module.apply(vars, x, jnp.array(0), mutable=\"cache\")\n",
    "fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = SHCSARow(n_cntx=n_cntx, n_feat = n_feat)\n",
    "y, _ = module.apply(vars, x, jnp.array(0), mutable=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = module.apply(vars, x, jnp.array(0), mutable=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.76740426,  0.45793283,  0.7712825 , -0.71566945, -1.3524578 ,\n",
       "       -0.02418369,  0.33236438, -0.27028525,  0.38507232,  0.7667557 ,\n",
       "        0.2532947 , -0.21232384, -1.6268221 , -0.52965856,  0.79835236,\n",
       "        1.1416652 , -0.0675956 , -0.43777275, -0.4003198 ,  1.12303   ,\n",
       "       -0.05400813,  0.42411083, -1.8518133 ,  1.2761084 , -1.313626  ,\n",
       "        0.08351888, -2.1435814 ,  1.9459411 , -0.9885833 ,  0.07802778,\n",
       "        0.9368881 ,  1.2056795 , -0.74474347,  0.74293506,  0.71277654,\n",
       "       -0.7897532 ,  0.46426433, -0.13851726, -1.3792179 ,  0.72289133,\n",
       "        0.39826477, -0.21821803,  0.9840089 , -0.5381244 ,  0.7619692 ,\n",
       "        0.31365275, -0.591218  ,  0.48691204,  0.8179485 , -0.8554354 ,\n",
       "        1.0123616 , -0.7641751 ,  0.3230685 ,  0.06167361,  1.16292   ,\n",
       "       -0.36336797, -0.5731621 ,  0.78331184,  0.821721  , -0.5655025 ,\n",
       "       -1.2870891 ,  0.7006638 ,  0.5295207 , -0.087327  ], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "kgpt = FGPT.MakeWithSHCSA(config, partial(SHCSARow, n_cntx=n_cntx, n_feat = n_feat))\n",
    "vars = kgpt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mkgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrng\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mPRNGKey\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f7406a63ac0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt_idxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogit_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mFGPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f740acd60e0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Auto-regressively generate tokens.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "variables : Dict\n",
      "    Variable dict for `apply` - should include the KV cache and model params.\n",
      "prompt_idxs : jax.Array\n",
      "    Prompt sequence as indicies into the vocabulary. E.g. [59423, 233, 921].\n",
      "logit_sampler : Callable\n",
      "    A function which accepts an array of logits of shape [n_embd] and return a\n",
      "    single token index.\n",
      "max_new_tokens : int\n",
      "    Max number of new tokens to generate.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "token_idx : jax.Array\n",
      "    The generated sequence of tokens - as vocab indicies. Shape \n",
      "    [len(prompt_idx) + max_new_tokens].\n",
      "\n",
      "Example usage::\n",
      "    \n",
      "        model = FGPT.Make(C)\n",
      "        vars = model.init_vars({\"params\": gpt_params})\n",
      "        token_idx = model.generate(vars, prompt_idx)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/nimbleGPT/nimblegpt/fast_model.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "kgpt.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"-inf\") * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Array([-6.51827455e-02, -1.64756298e+00,  8.68667841e-01,  1.08457220e+00,\n",
       "         -7.01299548e-01, -5.72350860e-01, -3.95007074e-01,  7.66254127e-01,\n",
       "         -7.61115551e-01,  1.54181850e+00,  1.33208990e-01, -1.61614037e+00,\n",
       "          1.48971975e-02,  6.72358334e-01, -1.21386719e+00, -5.99314332e-01,\n",
       "         -3.89698446e-01,  9.63883162e-01,  2.28442162e-01,  9.72948968e-02,\n",
       "         -1.19103360e+00,  6.08899295e-02,  9.51677740e-01,  9.33376431e-01,\n",
       "          1.76112592e-01, -4.33435768e-01,  3.72164249e-02,  3.05139422e-02,\n",
       "         -4.05094266e-01,  1.25109792e+00, -1.74354315e-02,  7.83537507e-01,\n",
       "         -3.58243942e-01,  1.08284950e-01, -7.79186428e-01,  9.94483590e-01,\n",
       "         -6.30619824e-01, -1.08396506e+00,  4.82888997e-01, -9.37316477e-01,\n",
       "         -9.34945345e-02, -3.83898556e-01,  5.94781935e-01,  1.69102609e-01,\n",
       "         -1.19115740e-01, -5.89539051e-01, -1.92776787e+00, -1.27550960e-03,\n",
       "          1.99623942e+00, -3.00156474e-01,  6.18577838e-01, -1.29633510e+00,\n",
       "          5.50791860e-01, -1.34302449e+00, -6.66098595e-01, -4.81662124e-01,\n",
       "          7.87017345e-02,  6.86272979e-01,  6.14920139e-01, -7.79183030e-01,\n",
       "          1.93121135e-01,  2.29605287e-03,  9.57537651e-01,  2.06121624e-01],      dtype=float32),\n",
       "  Array([[-0.00474644, -0.30927444,  0.50728333, ...,  0.39672348,\n",
       "           0.19067782,  0.30066544],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]], dtype=float32),\n",
       "  Array([[-0.76740426,  0.45793283,  0.7712825 , ...,  0.7006638 ,\n",
       "           0.5295207 , -0.087327  ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]], dtype=float32)),\n",
       " FrozenDict({\n",
       "     cache: {\n",
       "         cached_keys: Array([[-0.00474644, -0.30927444,  0.50728333, ...,  0.39672348,\n",
       "                  0.19067782,  0.30066544],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                ...,\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ]], dtype=float32),\n",
       "         cached_values: Array([[-0.76740426,  0.45793283,  0.7712825 , ...,  0.7006638 ,\n",
       "                  0.5295207 , -0.087327  ],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                ...,\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ],\n",
       "                [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "                  0.        ,  0.        ]], dtype=float32),\n",
       "     },\n",
       " }))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fshcsa_module.apply(vars, x, jnp.array(0), mutable=\"cache\", method=\"get_qKV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_att = torch.concatenate((torch.tensor([1]), torch.zeros(n_cntx - 1)))\n",
    "sm_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 64]),\n",
       " tensor([[ 1.7014, -0.1726, -1.6793,  ..., -0.7675, -0.1582, -0.8307],\n",
       "         [-0.3748, -0.1536,  0.6626,  ..., -1.0473, -0.4401,  1.9859],\n",
       "         [-0.9378, -1.0736, -1.6285,  ...,  0.8539, -0.6150, -1.1558],\n",
       "         ...,\n",
       "         [ 1.0742, -0.6377, -0.2379,  ..., -1.6558, -0.9871,  0.6652],\n",
       "         [-1.0223, -0.3294,  0.1540,  ...,  0.0216,  0.1360,  0.0924],\n",
       "         [ 1.4035,  1.7089, -2.7962,  ..., -1.1578, -0.3323,  0.0676]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.randn((n_cntx, n_feat))\n",
    "V.shape, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7014, -0.1726, -1.6793,  ..., -0.7675, -0.1582, -0.8307],\n",
       "        [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_att[:, None] * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7014, -0.1726, -1.6793, -2.0957,  0.7844, -1.2923, -1.1041,  1.0879,\n",
       "         0.3221,  0.9796,  0.2768,  0.6901,  0.0898, -0.4092, -0.2104, -0.5178,\n",
       "        -0.8111,  0.7136,  1.5241, -1.2647, -0.2747, -0.1076,  1.0560,  1.4402,\n",
       "        -1.5324, -0.1405,  0.6247,  1.4614,  0.4163,  1.4627, -0.7094,  1.5771,\n",
       "        -1.3416, -0.2051,  0.3650,  0.4769,  0.3391, -0.2098, -0.6240, -0.1273,\n",
       "         0.4322, -0.7927, -0.2393, -1.2307,  0.9321, -1.0986,  0.7401,  0.9525,\n",
       "         1.1932,  0.2744, -0.7200, -2.6377, -1.2093,  1.2528,  0.4161, -1.4054,\n",
       "        -0.4533, -0.7188, -0.4449,  0.8719,  0.9343, -0.7675, -0.1582, -0.8307])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(sm_att[:, None] * V, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
