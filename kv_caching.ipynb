{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to explore caching computed keys and values. Problems to solve:\n",
    "- How to use flax to store these as variables\n",
    "- Will they fit in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import lax\n",
    "from jax.tree_util import tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt import get_config_for, make_gpt_param_dict, get_flaxmodels_gpt2_params, param_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt.base_model import (\n",
    "    BaseBlock,\n",
    "    BaseCausalSelfAttention,\n",
    "    BaseGPT,\n",
    "    BaseSingleHeadCausalSelfAttention,\n",
    ")\n",
    "from nimblegpt.model import GELU, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attn_pdrop: 0.1\n",
       "block_size: 1024\n",
       "embd_pdrop: 0.1\n",
       "model_type: gpt2\n",
       "n_embd: 768\n",
       "n_head: 12\n",
       "n_layer: 12\n",
       "resid_pdrop: 0.1\n",
       "vocab_size: 50257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,572,864'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_feat = config.n_embd // config.n_head\n",
    "# Q/K/V are [config.block_size, n_feat]\n",
    "n_cache_params = config.block_size * n_feat * 2 * config.n_layer\n",
    "f\"{n_cache_params:,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K/V parameters for the entire context are on the order of 1 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadQKV(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute Q, K, V matrices for a single head.\n",
    "\n",
    "    This module processes a single token embedding at a time, and builds up a cache\n",
    "    of K and V matrices for the entire sequence. The caching implementation is based on:\n",
    "    https://flax.readthedocs.io/en/latest/_modules/flax/linen/attention.html#MultiHeadDotProductAttention\n",
    "    \"\"\"\n",
    "    n_cntx: int\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.Array\n",
    "            Shape [n_embd]. The token embedding for the next token in the sequence.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Q, K, V, idx : Tuple\n",
    "            Q : [n_feat] - The query vector for `x`.\n",
    "            K, V : [n_cntx, n_feat] - The key and value matrices for the entire context.\n",
    "            idx : int - The index of `x` in the context.\n",
    "        \"\"\"\n",
    "        # Attention q, k, v vectors for token embedding `x`. Shape [n_feat].\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=0)\n",
    "\n",
    "        is_initialized = self.has_variable(\"cache\", \"cached_keys\")\n",
    "\n",
    "        # Cached K and V matrices. Shape [n_cntx, n_feat].\n",
    "        cached_keys = self.variable(\"cache\", \"cached_keys\", jnp.zeros,\n",
    "                                    (self.n_cntx, self.n_feat))\n",
    "        cached_values = self.variable(\"cache\", \"cached_values\", jnp.zeros,\n",
    "                                      (self.n_cntx, self.n_feat))\n",
    "\n",
    "        cached_index = self.variable(\"cache\", \"cache_index\",\n",
    "                                     lambda: jnp.array(0, dtype=jnp.int32))\n",
    "        cur_index = cached_index.value\n",
    "\n",
    "        if is_initialized:\n",
    "\n",
    "            K = lax.dynamic_update_slice(cached_keys.value,\n",
    "                                         jnp.expand_dims(k, axis=0),\n",
    "                                         (cur_index, 0))\n",
    "            V = lax.dynamic_update_slice(cached_values.value,\n",
    "                                         jnp.expand_dims(v, axis=0),\n",
    "                                         (cur_index, 0))\n",
    "\n",
    "            cached_keys.value = K\n",
    "            cached_values.value = V\n",
    "            cached_index.value = cur_index + 1\n",
    "\n",
    "        return q, cached_keys.value, cached_values.value, cur_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_params = make_gpt_param_dict(get_flaxmodels_gpt2_params(), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_module = SingleHeadQKV(n_cntx=config.block_size, n_feat=config.n_embd // config.n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones((config.n_embd,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'kernel': '(768, 192)', 'bias': '(192)'}},\n",
       " 'cache': {'cached_keys': '(1024, 64)',\n",
       "  'cached_values': '(1024, 64)',\n",
       "  'cache_index': '()'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_shapes(qkv_module.init(jax.random.PRNGKey(0), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_0_params = gpt_params[\"Block_0\"][\"CausalSelfAttention_0\"][\"VmapSingleHeadCausalSelfAttention_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': '(12, 192)', 'kernel': '(12, 768, 192)'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_shapes(sa_0_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_0_params = tree_map(lambda x: x[0], sa_0_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': '(192)', 'kernel': '(768, 192)'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_shapes(sh_0_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jax.random.normal(rng, (10, config.n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_QKV = nn.Dense(features=3 * config.n_embd // config.n_head).apply({\"params\": sh_0_params[\"Dense_0\"]}, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  0.3254057 ,  -1.9289862 ,   6.3326793 , ...,  -0.500361  ,\n",
       "         -0.35731804,  -1.1551355 ],\n",
       "       [  4.2553654 ,   3.9190474 ,  -7.942846  , ...,  -0.02936717,\n",
       "         -0.8493566 ,   1.0613071 ],\n",
       "       [ -3.7252674 ,  -1.1315012 ,   8.182152  , ...,   1.7368824 ,\n",
       "         -0.6001127 ,  -0.4603993 ],\n",
       "       ...,\n",
       "       [  0.7600452 ,  -3.0968392 ,   5.0037227 , ...,   2.1742795 ,\n",
       "          4.5695148 ,  -0.9679753 ],\n",
       "       [-10.481102  ,  -1.1025255 ,  -9.36252   , ...,   0.1816734 ,\n",
       "          1.8518579 ,   0.6153525 ],\n",
       "       [  1.979913  ,  11.513443  ,   2.6524346 , ...,  -2.272161  ,\n",
       "         -0.8667084 ,  -4.240763  ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = qkv_module.init(rng, X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    qKVi, vars = qkv_module.apply({\"cache\": vars[\"cache\"], \"params\": sh_0_params}, X[i], mutable=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 1.9799105e+00,  1.1513445e+01,  2.6524339e+00, -1.3477521e+01,\n",
       "        -1.8126870e+00, -6.0371763e-01, -3.0165085e-01,  4.2052898e+00,\n",
       "        -3.4322548e+00,  5.6196337e+00, -5.6358824e+00, -6.2805634e+00,\n",
       "        -1.5989894e+00, -7.7256999e+00, -7.7426491e+00, -1.1914519e+01,\n",
       "         5.6751199e+00,  4.1370215e+00, -4.8818932e+00, -6.8391347e-01,\n",
       "         7.4022107e+00, -4.2368451e-01, -5.6376481e+00,  1.1128722e+01,\n",
       "        -7.7164817e-01, -2.1515315e+00, -6.5387106e-01,  1.2627184e+01,\n",
       "        -7.0974302e+00, -1.0442138e+01,  4.7516134e-01, -1.4268667e+00,\n",
       "         3.8699193e+00,  8.0294199e+00,  2.1324763e+00, -5.1904230e+00,\n",
       "         9.3552160e+00, -1.3107698e+01, -1.2407379e+00, -3.3083718e+00,\n",
       "        -1.4584163e+00, -1.2347947e+01, -4.5201941e+00,  4.4677892e+00,\n",
       "        -8.2197313e+00,  8.2265444e+00, -8.2702935e-04, -5.0314231e+00,\n",
       "        -1.6529402e+01, -8.2724028e+00, -7.8701715e+00,  3.3075047e+00,\n",
       "         4.2626238e+00,  1.0886194e+01,  8.8508015e+00,  1.6122013e+00,\n",
       "        -7.3190269e+00,  2.8449569e+00,  2.0385671e+00,  5.1680202e+00,\n",
       "        -7.9761915e+00,  5.3030863e+00,  5.8520374e+00,  6.9597106e+00],      dtype=float32),\n",
       " Array([[  0.04223603,   0.8799178 , -11.74339   , ...,  -9.586003  ,\n",
       "          -3.4196584 ,   0.6275733 ],\n",
       "        [ -2.908174  ,   1.6900772 ,   3.1685874 , ...,  -0.8879423 ,\n",
       "           6.6386213 ,   1.1736691 ],\n",
       "        [  7.2307673 ,   3.8176613 ,  -2.786204  , ...,   0.9499583 ,\n",
       "         -11.866965  ,   4.592053  ],\n",
       "        ...,\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ]], dtype=float32),\n",
       " Array([[-0.00377496, -1.0600836 , -1.6187843 , ..., -0.50036067,\n",
       "         -0.35731807, -1.1551358 ],\n",
       "        [ 0.73015064,  1.4414431 ,  2.4035974 , ..., -0.02936776,\n",
       "         -0.8493568 ,  1.0613067 ],\n",
       "        [-1.3625284 ,  0.21987209,  0.27310872, ...,  1.7368824 ,\n",
       "         -0.60011214, -0.46039972],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]], dtype=float32),\n",
       " Array(19, dtype=int32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qKVi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qKVi[0]\n",
    "k = qKVi[1][9]\n",
    "v = qKVi[2][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.861023e-06, dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sh_QKV[9] - jnp.concatenate([q, k, v])).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = jnp.split(sh_QKV, 3, axis=1)[1]\n",
    "V = jnp.split(sh_QKV, 3, axis=1)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  0.04223603,   0.8799178 , -11.74339   , ...,  -9.586003  ,\n",
       "         -3.4196584 ,   0.6275733 ],\n",
       "       [ -2.908174  ,   1.6900772 ,   3.1685874 , ...,  -0.8879423 ,\n",
       "          6.6386213 ,   1.1736691 ],\n",
       "       [  7.2307673 ,   3.8176613 ,  -2.786204  , ...,   0.9499583 ,\n",
       "        -11.866965  ,   4.592053  ],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qKVi[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4.7683716e-06, dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(K - qKVi[1][:10]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.3113022e-06, dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(V - qKVi[2][:10]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSingleHeadCausalSelfAttention(BaseSingleHeadCausalSelfAttention):\n",
    "    n_feat: int\n",
    "    n_cntx: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        C = x.shape  # Embedding dimensionality (n_embd).\n",
    "\n",
    "        \n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        # [T, n_feat] @ [n_feat, T] -> [T, T].\n",
    "        # Row i of att tells us which tokens x[i] should attend to. att[i][j]\n",
    "        # is high when token i should attend heavily to token j.\n",
    "        att = (q @ k.T) * (1.0 / jnp.sqrt(self.n_feat))\n",
    "\n",
    "        # Token i should not attend to token j for any j > i. We set att to -inf\n",
    "        # for any position above the diagonal - i.e. where j > i.\n",
    "        # Note that this also prevents data tokens from attending to padding tokens.\n",
    "        causal_mask = ~jnp.tril(jnp.ones((T, T))).astype(bool)\n",
    "        att = jnp.where(causal_mask, -jnp.inf, att)\n",
    "\n",
    "        att = softmax(att, axis=-1)\n",
    "\n",
    "        y = att @ v  # [T, T] @ [T, n_feat] -> [T, n_feat]\n",
    "\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
