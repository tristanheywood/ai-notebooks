{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`triton_call` cannot (currently) be vmapped - so we must write a multi-headed causal self\n",
    "attention kernel instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably write a faster version of `FSingleHeadCausalSelfAttention.get_qKV` using\n",
    "a Triton kernel:\n",
    "  - Linear layer Triton impl from https://github.com/ELS-RD/kernl/blob/main/src/kernl/implementations/linear_layer.py\n",
    "  - Use Triton to write back to the cache, rather than dynamic_update_slice\n",
    "  - Fuse the layer norm into the linear layer. Compute layer norm in a single pass using\n",
    "  https://github.com/ELS-RD/kernl/blob/main/src/kernl/implementations/layer_norm.py - then\n",
    "  pass the mean and variance into the linear layer impl."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer norm and linear layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a fused linear layer + embedding to q, k, v linear layer using Triton. Implement:\n",
    "\n",
    "- baseline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
