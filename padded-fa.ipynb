{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to implement flash attention, but save on memory acceses by ignoring padded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from flax import linen as nn\n",
    "from jax import lax\n",
    "\n",
    "from jax_triton import pallas as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt import get_config_for\n",
    "from nimblegpt.jmodel import JSingleHeadCausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "att = jax.random.normal(rng, (config.block_size,)*2)\n",
    "q_key, k_key, v_key = jax.random.split(rng, 3)\n",
    "q = jax.random.normal(q_key, (config.block_size, config.n_embd))\n",
    "k = jax.random.normal(k_key, (config.block_size, config.n_embd))\n",
    "v = jax.random.normal(v_key, (config.block_size, config.n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_attn_kernel(q_ref, k_ref, v_ref, p_ref, o_ref, *, seq_len: int, n_feat: int, n_ocols):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ------\n",
    "    q_ref, k_rev, v_ref: references to the Q, K, and V matrices.\n",
    "        All have shape shape [seq_len, n_feat].\n",
    "\n",
    "    Each kernel instances computes out[out_row_num, out_col_start: out_col_start + n_ocols].\n",
    "    This requires multiplying att[out_row_num, :] by v[:, out_col_start: out_col_start + n_ocols].\n",
    "\n",
    "    To compute att[out_row_num, :], we multiply q[out_row_num, :] by \n",
    "    k^T.\n",
    "    \"\"\"\n",
    "\n",
    "    # Each instance computes out[out_row_num, out_col_start: out_col_start + n_ocols]\n",
    "    out_row_num = pl.program_id(0)\n",
    "    out_col_start = pl.program_id(1) * n_ocols\n",
    "    n_padd = p_ref[()]\n",
    "\n",
    "    seq_idxs = jnp.arange(seq_len)\n",
    "\n",
    "    # Shape (1,) mask. 0 if this instance is computing a padding element of `out`.\n",
    "    padd_row_mask = out_row_num >= n_padd\n",
    "    # Shape (seq_len,) mask. 0 for tokens of the sequence that are padding.\n",
    "    seq_mask = jnp.arange(seq_len) >= n_padd\n",
    "    # Shape (seq_len, n_ocols) mask. 0 for elements corresponding to padding tokens.\n",
    "    block_mask = lax.broadcast_in_dim(\n",
    "        jnp.expand_dims(seq_mask, 1), (seq_len, n_ocols), (0, 1)\n",
    "    )\n",
    "    # Shape (seq_len, n_feat) mask. 0 for elements corresponding to padding tokens.\n",
    "    mat_mask = lax.broadcast_in_dim(\n",
    "        jnp.expand_dims(seq_mask, 1), (seq_len, n_feat), (0, 1)\n",
    "    )\n",
    "    # Token i should only atten to tokens j <= i. 0 for tokens j > i.\n",
    "    causal_mask = seq_idxs <= out_row_num\n",
    "\n",
    "    ### First we compute the softmax of row `out_row_num` of the attention matrix. ###\n",
    "    # This requires loading one row of Q and all of K.\n",
    "\n",
    "    q_idx = (out_row_num, pl.dslice(None))\n",
    "    q_row = pl.load(q_ref, q_idx, mask=padd_row_mask, other=0) # [n_feat]\n",
    "    q_row = jnp.expand_dims(q_row, 0) # [1, n_feat]\n",
    "\n",
    "    k_idx = (pl.dslice(None), pl.dslice(None))\n",
    "    k_mat = pl.load(k_ref, k_idx, mask=mat_mask, other=0) # [seq_len, n_feat]\n",
    "\n",
    "    # Compute att[out_row_num, :] - a single row of the full attention matrix.\n",
    "    # [1, n_feat] . ([seq_len, n_feat] -[T]-> [n_feat, seq_len]) = [1, seq_len]\n",
    "    att_row = pl.dot(q_row, k_mat, trans_b = True)\n",
    "    att_row /= jnp.sqrt(n_feat)\n",
    "    att_row = jnp.where(causal_mask & seq_mask, att_row, -jnp.inf)\n",
    "    sm_numerator = jnp.exp(att_row - jnp.max(att_row))\n",
    "    sm_att = sm_numerator / jnp.sum(sm_numerator, keepdims=True) # [1, seq_len]\n",
    "\n",
    "    v_idxs = (pl.dslice(None), pl.dslice(out_col_start, n_ocols))\n",
    "    v_block = pl.load(v_ref, v_idxs, mask=block_mask, other=0) # [seq_len, n_ocols]\n",
    "\n",
    "    # [1, seq_len] . [seq_len, n_ocols] = [1, n_ocols]\n",
    "    # out = pl.dot(sm_att, v_block) # [1, n_ocols]\n",
    "    out = sm_att @ v_block\n",
    "\n",
    "    # Store the result.\n",
    "    out_idxs = (out_row_num, pl.dslice(out_col_start, n_ocols))\n",
    "    pl.store(o_ref, out_idxs, out[0], mask=padd_row_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_attn(q, k, v, n_padd, *, n_ocols: int):\n",
    "    seq_len, n_feat = q.shape\n",
    "\n",
    "    grid = (seq_len, n_feat // n_ocols)\n",
    "    assert grid[1] * n_ocols == n_feat\n",
    "\n",
    "    kernel = functools.partial(\n",
    "        padded_attn_kernel, seq_len=seq_len, n_feat=n_feat, n_ocols=n_ocols\n",
    "    )\n",
    "    out_shape = jax.ShapeDtypeStruct((seq_len, n_feat), q.dtype)\n",
    "\n",
    "    out = pl.pallas_call(kernel, grid=grid, out_shape=out_shape, interpret=True)(\n",
    "        q, k, v, n_padd\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSingleHeadCausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inference only (no dropout) single headed attention.\n",
    "\n",
    "    minGPT docstring\n",
    "    ----------------\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, n_padd: int = 0):\n",
    "        T, C = x.shape  # sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        y = padded_attn(q, k, v, n_padd, n_ocols=4)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(rng, (config.block_size, config.n_embd))\n",
    "n_padd = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = JSingleHeadCausalSelfAttention(config.n_embd).init_with_output(rng, x, n_padd=n_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty, _ = TSingleHeadCausalSelfAttention(config.n_embd).init_with_output(rng, x, n_padd=n_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.4305115e-06, dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y[n_padd:] - ty[n_padd:]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.01607285e-01,  3.23455304e-01, -1.14697017e-01, ...,\n",
       "         3.42727363e-01, -1.50065219e+00,  4.98529613e-01],\n",
       "       [-1.88820034e-01,  2.30207741e-01, -5.91809638e-02, ...,\n",
       "         9.28151608e-02, -1.45710719e+00,  4.99784201e-01],\n",
       "       [-1.72552958e-01, -4.69248593e-01, -1.03122219e-01, ...,\n",
       "        -9.16234404e-02, -1.25068820e+00,  2.55948424e-01],\n",
       "       ...,\n",
       "       [ 4.17719148e-02, -6.57780915e-02, -6.61965087e-02, ...,\n",
       "        -3.43253762e-02,  4.40334566e-02,  4.70811427e-02],\n",
       "       [ 5.38661331e-02, -3.08343675e-02, -2.44537331e-02, ...,\n",
       "         3.44744660e-02,  1.47196651e-03,  2.74988767e-02],\n",
       "       [ 4.30060774e-02, -1.43105257e-03, -1.85891725e-02, ...,\n",
       "         4.80973572e-02,  9.45648737e-03,  4.85833324e-02]],      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_attn(q, k, v, 0, n_ocols = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3306000817.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    x =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0:1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.expand_dims(q[0], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (1,) and (768,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pl\u001b[39m.\u001b[39;49mdot(jnp\u001b[39m.\u001b[39;49mexpand_dims(q[\u001b[39m0\u001b[39;49m], \u001b[39m1\u001b[39;49m), k, trans_b\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/jax-triton/jax_triton/pallas/primitives.py:480\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(a, b, trans_a, trans_b, allow_tf32)\u001b[0m\n\u001b[1;32m    478\u001b[0m rhs_contract_dim \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(trans_b)\n\u001b[1;32m    479\u001b[0m lhs_contract_dim \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mnot\u001b[39;00m trans_a)\n\u001b[0;32m--> 480\u001b[0m \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39;49mlax\u001b[39m.\u001b[39;49mdot_general(\n\u001b[1;32m    481\u001b[0m     a, b, dimension_numbers\u001b[39m=\u001b[39;49m(((lhs_contract_dim,), (rhs_contract_dim,)), ((), ())),\n\u001b[1;32m    482\u001b[0m     precision\u001b[39m=\u001b[39;49mlax\u001b[39m.\u001b[39;49mPrecision\u001b[39m.\u001b[39;49mHIGH \u001b[39mif\u001b[39;49;00m allow_tf32 \u001b[39melse\u001b[39;49;00m lax\u001b[39m.\u001b[39;49mPrecision\u001b[39m.\u001b[39;49mHIGHEST,\n\u001b[1;32m    483\u001b[0m     preferred_element_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/lax/lax.py:768\u001b[0m, in \u001b[0;36mdot_general\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    763\u001b[0m bdims \u001b[39m=\u001b[39m (api_util\u001b[39m.\u001b[39m_ensure_index_tuple(lhs_batch),\n\u001b[1;32m    764\u001b[0m          api_util\u001b[39m.\u001b[39m_ensure_index_tuple(rhs_batch))\n\u001b[1;32m    765\u001b[0m preferred_element_type \u001b[39m=\u001b[39m (\n\u001b[1;32m    766\u001b[0m     \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m preferred_element_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(np\u001b[39m.\u001b[39mdtype(preferred_element_type)))\n\u001b[0;32m--> 768\u001b[0m \u001b[39mreturn\u001b[39;00m dot_general_p\u001b[39m.\u001b[39;49mbind(lhs, rhs,\n\u001b[1;32m    769\u001b[0m                           dimension_numbers\u001b[39m=\u001b[39;49m(cdims, bdims),\n\u001b[1;32m    770\u001b[0m                           precision\u001b[39m=\u001b[39;49mcanonicalize_precision(precision),\n\u001b[1;32m    771\u001b[0m                           preferred_element_type\u001b[39m=\u001b[39;49mpreferred_element_type)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/core.py:329\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    327\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    328\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 329\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/core.py:332\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 332\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    333\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/core.py:712\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 712\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/dispatch.py:118\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_primitive\u001b[39m(prim, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    117\u001b[0m   \u001b[39m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m   compiled_fun \u001b[39m=\u001b[39m xla_primitive_callable(prim, \u001b[39m*\u001b[39;49munsafe_map(arg_spec, args),\n\u001b[1;32m    119\u001b[0m                                         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    120\u001b[0m   \u001b[39mreturn\u001b[39;00m compiled_fun(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/util.py:254\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/util.py:247\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 247\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/dispatch.py:202\u001b[0m, in \u001b[0;36mxla_primitive_callable\u001b[0;34m(prim, *arg_specs, **params)\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mreturn\u001b[39;00m out,\n\u001b[0;32m--> 202\u001b[0m compiled \u001b[39m=\u001b[39m _xla_callable_uncached(lu\u001b[39m.\u001b[39;49mwrap_init(prim_fun), device, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    203\u001b[0m                                   prim\u001b[39m.\u001b[39;49mname, donated_invars, \u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49marg_specs)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[1;32m    205\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: compiled(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/dispatch.py:357\u001b[0m, in \u001b[0;36m_xla_callable_uncached\u001b[0;34m(fun, device, backend, name, donated_invars, keep_unused, *arg_specs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_xla_callable_uncached\u001b[39m(fun: lu\u001b[39m.\u001b[39mWrappedFun, device, backend, name,\n\u001b[1;32m    355\u001b[0m                            donated_invars, keep_unused, \u001b[39m*\u001b[39marg_specs):\n\u001b[1;32m    356\u001b[0m   \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mjax_array:\n\u001b[0;32m--> 357\u001b[0m     computation \u001b[39m=\u001b[39m sharded_lowering(fun, device, backend, name, donated_invars,\n\u001b[1;32m    358\u001b[0m                                    \u001b[39mFalse\u001b[39;49;00m, keep_unused, \u001b[39m*\u001b[39;49marg_specs)\n\u001b[1;32m    359\u001b[0m     \u001b[39mreturn\u001b[39;00m computation\u001b[39m.\u001b[39mcompile(_allow_propagation_to_outputs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39munsafe_call\n\u001b[1;32m    360\u001b[0m   \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/dispatch.py:348\u001b[0m, in \u001b[0;36msharded_lowering\u001b[0;34m(fun, device, backend, name, donated_invars, always_lower, keep_unused, *arg_specs)\u001b[0m\n\u001b[1;32m    343\u001b[0m in_shardings \u001b[39m=\u001b[39m [pxla\u001b[39m.\u001b[39m_UNSPECIFIED \u001b[39mif\u001b[39;00m i \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m in_shardings]\n\u001b[1;32m    345\u001b[0m \u001b[39m# Pass in a singleton `_UNSPECIFIED` for out_shardings because we don't know\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# the number of output avals at this stage. lower_sharding_computation will\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m# apply it to all out_avals.\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[39mreturn\u001b[39;00m pxla\u001b[39m.\u001b[39;49mlower_sharding_computation(\n\u001b[1;32m    349\u001b[0m     fun, \u001b[39m'\u001b[39;49m\u001b[39mjit\u001b[39;49m\u001b[39m'\u001b[39;49m, name, in_shardings, pjit\u001b[39m.\u001b[39;49m_UNSPECIFIED, donated_invars,\n\u001b[1;32m    350\u001b[0m     in_avals, in_is_global\u001b[39m=\u001b[39;49m(\u001b[39mTrue\u001b[39;49;00m,) \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(arg_specs), keep_unused\u001b[39m=\u001b[39;49mkeep_unused,\n\u001b[1;32m    351\u001b[0m     always_lower\u001b[39m=\u001b[39;49malways_lower, devices_from_context\u001b[39m=\u001b[39;49mda)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/interpreters/pxla.py:2792\u001b[0m, in \u001b[0;36mlower_sharding_computation\u001b[0;34m(fun, api_name, fun_name, in_shardings, out_shardings, donated_invars, global_in_avals, in_is_global, keep_unused, always_lower, devices_from_context)\u001b[0m\n\u001b[1;32m   2787\u001b[0m name_stack \u001b[39m=\u001b[39m new_name_stack(wrap_name(fun_name, api_name))\n\u001b[1;32m   2789\u001b[0m \u001b[39mwith\u001b[39;00m dispatch\u001b[39m.\u001b[39mlog_elapsed_time(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinished tracing + transforming \u001b[39m\u001b[39m{\u001b[39;00mname_stack\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2790\u001b[0m                                \u001b[39m\"\u001b[39m\u001b[39min \u001b[39m\u001b[39m{elapsed_time}\u001b[39;00m\u001b[39m sec\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2791\u001b[0m                                event\u001b[39m=\u001b[39mdispatch\u001b[39m.\u001b[39mJAXPR_TRACE_EVENT):\n\u001b[0;32m-> 2792\u001b[0m   jaxpr, global_out_avals, consts \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39;49mtrace_to_jaxpr_final(\n\u001b[1;32m   2793\u001b[0m       fun, global_in_avals, debug_info\u001b[39m=\u001b[39;49mpe\u001b[39m.\u001b[39;49mdebug_info_final(fun, api_name))\n\u001b[1;32m   2794\u001b[0m kept_outputs \u001b[39m=\u001b[39m [\u001b[39mTrue\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(global_out_avals)\n\u001b[1;32m   2796\u001b[0m \u001b[39mif\u001b[39;00m _is_unspecified(out_shardings):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/interpreters/partial_eval.py:2065\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_final\u001b[0;34m(fun, in_avals, debug_info, keep_inputs)\u001b[0m\n\u001b[1;32m   2063\u001b[0m   main\u001b[39m.\u001b[39mjaxpr_stack \u001b[39m=\u001b[39m ()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m   \u001b[39mwith\u001b[39;00m core\u001b[39m.\u001b[39mnew_sublevel():\n\u001b[0;32m-> 2065\u001b[0m     jaxpr, out_avals, consts \u001b[39m=\u001b[39m trace_to_subjaxpr_dynamic(\n\u001b[1;32m   2066\u001b[0m       fun, main, in_avals, keep_inputs\u001b[39m=\u001b[39;49mkeep_inputs, debug_info\u001b[39m=\u001b[39;49mdebug_info)\n\u001b[1;32m   2067\u001b[0m   \u001b[39mdel\u001b[39;00m fun, main\n\u001b[1;32m   2068\u001b[0m \u001b[39mreturn\u001b[39;00m jaxpr, out_avals, consts\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/interpreters/partial_eval.py:1998\u001b[0m, in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs, debug_info)\u001b[0m\n\u001b[1;32m   1996\u001b[0m in_tracers \u001b[39m=\u001b[39m _input_type_to_tracers(trace\u001b[39m.\u001b[39mnew_arg, in_avals)\n\u001b[1;32m   1997\u001b[0m in_tracers_ \u001b[39m=\u001b[39m [t \u001b[39mfor\u001b[39;00m t, keep \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(in_tracers, keep_inputs) \u001b[39mif\u001b[39;00m keep]\n\u001b[0;32m-> 1998\u001b[0m ans \u001b[39m=\u001b[39m fun\u001b[39m.\u001b[39;49mcall_wrapped(\u001b[39m*\u001b[39;49min_tracers_)\n\u001b[1;32m   1999\u001b[0m out_tracers \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(trace\u001b[39m.\u001b[39mfull_raise, ans)\n\u001b[1;32m   2000\u001b[0m jaxpr, consts \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mto_jaxpr(out_tracers)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/linear_util.py:167\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m gen \u001b[39m=\u001b[39m gen_static_args \u001b[39m=\u001b[39m out_store \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m   ans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    168\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m   \u001b[39m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    170\u001b[0m   \u001b[39m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[39m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[39m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    173\u001b[0m   \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m   \u001b[39mwhile\u001b[39;00m stack:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/dispatch.py:197\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.prim_fun\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprim_fun\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 197\u001b[0m   out \u001b[39m=\u001b[39m prim\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    198\u001b[0m   \u001b[39mif\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/core.py:329\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    327\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    328\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 329\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/core.py:332\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 332\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    333\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/interpreters/partial_eval.py:1713\u001b[0m, in \u001b[0;36mDynamicJaxprTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[39mif\u001b[39;00m primitive \u001b[39min\u001b[39;00m custom_staging_rules:\n\u001b[1;32m   1712\u001b[0m   \u001b[39mreturn\u001b[39;00m custom_staging_rules[primitive](\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtracers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m-> 1713\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_process_primitive(primitive, tracers, params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/interpreters/partial_eval.py:1717\u001b[0m, in \u001b[0;36mDynamicJaxprTrace.default_process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_process_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[1;32m   1716\u001b[0m   avals \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39maval \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tracers]\n\u001b[0;32m-> 1717\u001b[0m   out_avals, effects \u001b[39m=\u001b[39m primitive\u001b[39m.\u001b[39;49mabstract_eval(\u001b[39m*\u001b[39;49mavals, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m   1718\u001b[0m   out_avals \u001b[39m=\u001b[39m [out_avals] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m primitive\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m out_avals\n\u001b[1;32m   1719\u001b[0m   source_info \u001b[39m=\u001b[39m source_info_util\u001b[39m.\u001b[39mcurrent()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/core.py:365\u001b[0m, in \u001b[0;36m_effect_free_abstract_eval.<locals>.abstract_eval_\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mabstract_eval_\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 365\u001b[0m   \u001b[39mreturn\u001b[39;00m abstract_eval(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), no_effects\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/lax/utils.py:66\u001b[0m, in \u001b[0;36mstandard_abstract_eval\u001b[0;34m(prim, shape_rule, dtype_rule, weak_type_rule, named_shape_rule, *avals, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39mConcreteArray(out\u001b[39m.\u001b[39mdtype, out, weak_type\u001b[39m=\u001b[39mweak_type)\n\u001b[1;32m     65\u001b[0m \u001b[39melif\u001b[39;00m least_specialized \u001b[39mis\u001b[39;00m core\u001b[39m.\u001b[39mShapedArray:\n\u001b[0;32m---> 66\u001b[0m   \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39mShapedArray(shape_rule(\u001b[39m*\u001b[39;49mavals, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[1;32m     67\u001b[0m                           dtype_rule(\u001b[39m*\u001b[39mavals, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), weak_type\u001b[39m=\u001b[39mweak_type,\n\u001b[1;32m     68\u001b[0m                           named_shape\u001b[39m=\u001b[39mnamed_shape_rule(\u001b[39m*\u001b[39mavals, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[1;32m     69\u001b[0m \u001b[39melif\u001b[39;00m least_specialized \u001b[39mis\u001b[39;00m core\u001b[39m.\u001b[39mDShapedArray:\n\u001b[1;32m     70\u001b[0m   shape \u001b[39m=\u001b[39m shape_rule(\u001b[39m*\u001b[39mavals, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/lax/lax.py:2530\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2527\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m core\u001b[39m.\u001b[39msymbolic_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   2528\u001b[0m   msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2529\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mshape, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2530\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   2532\u001b[0m \u001b[39mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[39m.\u001b[39mshape, rhs\u001b[39m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (1,) and (768,)."
     ]
    }
   ],
   "source": [
    "pl.dot(jnp.expand_dims(q[0], 1), k, trans_b=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
