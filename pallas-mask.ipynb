{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from flax import linen as nn\n",
    "from jax import lax\n",
    "\n",
    "from jax_triton import pallas as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt import get_config_for\n",
    "from nimblegpt.jmodel import JSingleHeadCausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "att = jax.random.normal(rng, (config.block_size,)*2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padded Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_softmax(block_size: int, num_warps: int = 1):\n",
    "    \"\"\"\n",
    "    Returned kernel has signature:\n",
    "    def padded_softmax_kernel(att: Array[block_size, block_size], n_padd: int):\n",
    "\n",
    "    Such that `padded_softmax_kernel(att, n_padd)` only returns a correct result for\n",
    "    rows which don't correspond to padding tokens.\n",
    "    \"\"\"\n",
    "    # grid = block_size => one kernel instance per row of the input matrix.\n",
    "    @functools.partial(\n",
    "        pl.pallas_call,\n",
    "        out_shape=jax.ShapeDtypeStruct((block_size, block_size), jnp.float32),\n",
    "        grid=block_size,\n",
    "        num_warps=num_warps,\n",
    "        interpret=True,\n",
    "        debug=False\n",
    "    )\n",
    "    def padded_softmax_kernel(x_ref, p_ref, o_ref):\n",
    "        row_idx = pl.program_id(0)\n",
    "        n_padd = p_ref[()]\n",
    "\n",
    "        x_idx = jnp.arange(block_size)\n",
    "        row_idxs = (row_idx, x_idx)\n",
    "\n",
    "        # 1 for valid elements of `x_ref`, 0 elsewhere (i.e. out of bounds).\n",
    "        valid_mask = x_idx < x_ref.shape[1]\n",
    "\n",
    "        # Token i should only attend to tokens j <= i.\n",
    "        causal_mask = x_idx <= row_idx\n",
    "\n",
    "        # 1 in the bottom right corner of the matrix - where data tokens attend to data\n",
    "        # tokens. 0 elsewhere.\n",
    "        padd_mask = (x_idx >= n_padd) & (row_idx >= n_padd)\n",
    "\n",
    "        read_mask = valid_mask & causal_mask & padd_mask\n",
    "        row = pl.load(x_ref, row_idxs, mask=read_mask, other=-float(\"inf\"))\n",
    "\n",
    "        row_minus_max = row - jnp.max(row, axis=0)\n",
    "        numerator = jnp.exp(row_minus_max)\n",
    "        denominator = jnp.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "\n",
    "        # Only write back to rows corresponding to non-padding tokens. Padding tokens\n",
    "        # may be uninitialized memory.\n",
    "        pl.store(o_ref, row_idxs, softmax_output, mask=valid_mask & (row_idx >= n_padd))\n",
    "\n",
    "    return padded_softmax_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSingleHeadCausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inference only (no dropout) single headed attention.\n",
    "\n",
    "    minGPT docstring\n",
    "    ----------------\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, n_padd: int = 0):\n",
    "        T, C = x.shape  # sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        # [T, n_feat] @ [n_feat, T] -> [T, T].\n",
    "        # Row i of att tells us which tokens x[i] should attend to. att[i][j]\n",
    "        # is high when token i should attend heavily to token j.\n",
    "        att = (q @ k.T) * (1.0 / jnp.sqrt(self.n_feat))\n",
    "\n",
    "        att = make_padded_softmax(T)(att, n_padd)\n",
    "\n",
    "        y = att @ v  # [T, T] @ [T, n_feat] -> [T, n_feat]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(rng, (config.block_size, config.n_embd))\n",
    "n_padd = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = JSingleHeadCausalSelfAttention(config.n_embd).init_with_output(rng, x, n_padd=n_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty, _ = TSingleHeadCausalSelfAttention(config.n_embd).init_with_output(rng, x, n_padd=n_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0., dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We dont care about the embeddings for padding tokens.\n",
    "(y[n_padd:] - ty[n_padd:]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.1136408 ,  1.2468629 , -0.99344945, ...,  0.33121186,\n",
       "         1.0703579 , -0.06721891],\n",
       "       [-0.8053709 ,  0.7387072 , -0.53019696, ...,  0.30776542,\n",
       "         0.9296782 , -0.15966392],\n",
       "       [-0.9898438 ,  1.0326734 , -0.8366718 , ...,  0.26767203,\n",
       "         0.93346596, -0.0699129 ],\n",
       "       ...,\n",
       "       [ 0.0416423 ,  0.06101406, -0.04523662, ..., -0.06422241,\n",
       "        -0.04720704,  0.02136195],\n",
       "       [ 0.03843816,  0.01985435,  0.04832299, ..., -0.09897571,\n",
       "         0.04451201,  0.0089184 ],\n",
       "       [ 0.00146818,  0.04245059,  0.07542939, ...,  0.01340979,\n",
       "         0.08863425,  0.00376507]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[n_padd:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.1136408 ,  1.2468629 , -0.99344945, ...,  0.33121186,\n",
       "         1.0703579 , -0.06721891],\n",
       "       [-0.8053709 ,  0.7387072 , -0.53019696, ...,  0.30776542,\n",
       "         0.9296782 , -0.15966392],\n",
       "       [-0.9898438 ,  1.0326734 , -0.8366718 , ...,  0.26767203,\n",
       "         0.93346596, -0.0699129 ],\n",
       "       ...,\n",
       "       [ 0.0416423 ,  0.06101406, -0.04523662, ..., -0.06422241,\n",
       "        -0.04720704,  0.02136195],\n",
       "       [ 0.03843816,  0.01985435,  0.04832299, ..., -0.09897571,\n",
       "         0.04451201,  0.0089184 ],\n",
       "       [ 0.00146818,  0.04245059,  0.07542939, ...,  0.01340979,\n",
       "         0.08863425,  0.00376507]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty[n_padd:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V + Padded Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_softmax_v(seq_len: int, n_feat: int, n_ocols, num_warps: int = 1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq_len\n",
    "      GPT context length (1024)\n",
    "    n_feat\n",
    "      Number of features per q/k/v matrix, per attention head (64)\n",
    "    n_ocols\n",
    "      Number of columns of output to calculate per kernel instance. The full size\n",
    "      of the output will be [block_size, n_feat], so n_ocols shuld divide n_feat.\n",
    "\n",
    "    Returned kernel has signature:\n",
    "    def padded_softmax_kernel(att: Array[block_size, block_size], v: Array[block_size, n_feat], n_padd: int):\n",
    "\n",
    "    Such that `padded_softmax_kernel(att, n_padd)` only returns a correct result for\n",
    "    rows which don't correspond to padding tokens.\n",
    "\n",
    "    Kernel with grid index (i, j) is responsible for block out[i, (j:j+1)*n_ocols].\n",
    "    To compute this is we must read att[i, :] and v[:, (j:j+1)*n_ocols].\n",
    "    \"\"\"\n",
    "    # grid = (block_size, n_vblocks) => one kernel per [1, n_vblocks] block of the output matrix.\n",
    "    @functools.partial(\n",
    "        pl.pallas_call,\n",
    "        out_shape=jax.ShapeDtypeStruct((seq_len, n_feat), jnp.float32),\n",
    "        grid=(seq_len, n_feat // n_ocols),\n",
    "        num_warps=num_warps,\n",
    "        debug=False,\n",
    "        interpret=True,\n",
    "    )\n",
    "    def padded_softmax_v_kernel(att_ref, v_ref, p_ref, o_ref):\n",
    "        # Row of attention matrix that this kernel instance will process.\n",
    "        att_row_num = pl.program_id(0)\n",
    "        # Start of the block of columns of the `v` matrix that this kernel instance will process.\n",
    "        v_col_start = pl.program_id(1) * n_ocols\n",
    "        n_padd = p_ref[()]\n",
    "\n",
    "        ### Create indicies for reading memory. ###\n",
    "        seq_idxs = jnp.arange(seq_len)\n",
    "\n",
    "        att_idxs = (att_row_num, pl.dslice(None))\n",
    "\n",
    "        ## [seq_len,] mask.\n",
    "        # Token i should only attend to tokens j <= i.\n",
    "        causal_mask = seq_idxs <= att_row_num\n",
    "        padd_from_mask = (\n",
    "            seq_idxs >= n_padd\n",
    "        )  # 0 when padding tokens are attending to anything.\n",
    "        padd_to_mask = (\n",
    "            att_row_num >= n_padd\n",
    "        )  # 0 when anything is attending to padding tokens.\n",
    "        padd_mask = padd_from_mask & padd_to_mask\n",
    "        seq_mask = causal_mask & padd_mask\n",
    "\n",
    "        ## Index for v[:, (j:j+1)*n_ocols].\n",
    "        v_col_idxs = pl.dslice(v_col_start, n_ocols)\n",
    "        v_row_idxs = pl.dslice(0, seq_len)\n",
    "        v_idxs = (v_row_idxs, v_col_idxs)\n",
    "\n",
    "        ## Only read elements of `v` which will be multipled by non-padding tokens.\n",
    "        v_row_mask = padd_from_mask\n",
    "        v_mask = lax.broadcast_in_dim(\n",
    "            jnp.expand_dims(v_row_mask, 1), (seq_len, n_ocols), (0, 1)\n",
    "        )\n",
    "\n",
    "        out_idxs = (att_row_num, pl.dslice(v_col_start, n_ocols))\n",
    "\n",
    "        ### Compute attn row softmax. ###\n",
    "        att_row = pl.load(att_ref, att_idxs, mask=seq_mask, other=-float(\"inf\"))\n",
    "\n",
    "        numerator = jnp.exp(att_row - jnp.max(att_row, axis=0))\n",
    "        sma_row = numerator / jnp.sum(numerator, axis=0)\n",
    "\n",
    "        ### Multiply attention by `v`. ###\n",
    "        v_block = pl.load(v_ref, v_idxs, mask=v_mask, other=0)\n",
    "\n",
    "        # We want to do `out = sma_row @ v_block` ([seq_len,] @ [seq_len, n_ocols] => [n_ocols,])\n",
    "        # But Triton doesn't support matrix multiplication for small matrices.\n",
    "\n",
    "        # Poor man's matrix multiplication (may be slowing us down since it doesn't use tensor cores).\n",
    "        sma_mat = jnp.expand_dims(sma_row, 1)  # [seq_len, 1]\n",
    "        # [seq_len, 1] * [seq_len, n_ocols] -> [seq_len, n_ocols] -[sum]-> [n_ocols,]\n",
    "        out = jnp.sum(sma_mat * v_block, axis=0)\n",
    "\n",
    "        ### Write output. ###\n",
    "        pl.store(o_ref, out_idxs, out)\n",
    "\n",
    "    return padded_softmax_v_kernel\n",
    "\n",
    "\n",
    "class VTSingleHeadCausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inference only (no dropout) single headed attention.\n",
    "\n",
    "    minGPT docstring\n",
    "    ----------------\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, n_padd: int = 0):\n",
    "        T, C = x.shape  # sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        # [T, n_feat] @ [n_feat, T] -> [T, T].\n",
    "        # Row i of att tells us which tokens x[i] should attend to. att[i][j]\n",
    "        # is high when token i should attend heavily to token j.\n",
    "        att = (q @ k.T) * (1.0 / jnp.sqrt(self.n_feat))\n",
    "\n",
    "        y = make_padded_softmax_v(T, self.n_feat, n_ocols=4)(att, v, n_padd)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = jax.random.normal(rng, (config.block_size, config.block_size))\n",
    "v = jax.random.normal(rng, (config.block_size, config.n_embd))\n",
    "n_padd = 2\n",
    "x = jax.random.normal(rng, (config.block_size, config.n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = JSingleHeadCausalSelfAttention(config.n_embd).init_with_output(rng, x, n_padd=n_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vy, _ = VTSingleHeadCausalSelfAttention(config.n_embd).init_with_output(rng, x, n_padd=n_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4.7683716e-07, dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y[n_padd:] - vy[n_padd:]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.4681816e-03,  4.2450592e-02,  7.5429395e-02, -9.1652356e-02,\n",
       "       -3.1681035e-02,  1.0414794e-04,  3.7798032e-02, -1.1124283e-02,\n",
       "        1.1266769e-01, -1.3866793e-01], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.46817416e-03,  4.24505323e-02,  7.54294395e-02, -9.16523561e-02,\n",
       "       -3.16810384e-02,  1.04149804e-04,  3.77980322e-02, -1.11242868e-02,\n",
       "        1.12667724e-01, -1.38667867e-01], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vy[-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[False, False, False, False,  True],\n",
       "       [False, False, False, False,  True],\n",
       "       [False, False, False, False,  True],\n",
       "       [False, False, False, False,  True]], dtype=bool)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.tile(jnp.arange(5) > 3, (4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[False, False, False, False,  True],\n",
       "       [False, False, False, False,  True],\n",
       "       [False, False, False, False,  True],\n",
       "       [False, False, False, False,  True]], dtype=bool)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.lax.broadcast(jnp.arange(5) > 3, (4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lax.broadcast_in_dim(jnp.expand_dims(jnp.arange(5) >= 3, 1,), (5, 4), (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = make_padded_softmax_v(config.block_size, config.n_embd, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[            nan,             nan,             nan, ...,\n",
       "                    nan,             nan,             nan],\n",
       "       [            nan,             nan,             nan, ...,\n",
       "                    nan,             nan,             nan],\n",
       "       [-6.50894493e-02,  1.23105273e-01,  1.24469054e+00, ...,\n",
       "         6.53849840e-01,  6.03153765e-01,  1.89329430e-01],\n",
       "       ...,\n",
       "       [ 5.01023568e-02, -4.25007492e-02,  3.23878042e-02, ...,\n",
       "        -1.46954395e-02, -7.23278970e-02, -5.75641170e-02],\n",
       "       [-6.83210790e-04, -2.03592703e-03,  1.18623391e-01, ...,\n",
       "        -3.67972404e-02,  4.68270928e-02,  1.81004982e-02],\n",
       "       [ 1.67859476e-02, -3.88669893e-02,  3.94940078e-02, ...,\n",
       "         4.67684865e-02, -5.01697585e-02,  9.15485024e-02]],      dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel(att, v, n_padd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
