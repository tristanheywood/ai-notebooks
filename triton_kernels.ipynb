{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re implement the pallas kernels from 'shcsa_kernels.py` in pure Triton, and call them\n",
    "with `triton_call`. This allows us to use the 'mlir' branch of Triton, since pallas\n",
    "support is still a work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax_triton as jt\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimblegpt.model import SingleHeadCausalSelfAttention, softmax\n",
    "from nimblegpt import get_config_for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement basic (no padding) softmax in Triton - to start simply."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax from the Triton examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    input_ptr, output_ptr, input_row_stride, output_row_stride, n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    # The rows of the softmax are independent, so we parallelize across those\n",
    "    row_idx = tl.program_id(0)\n",
    "    # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "    # row in a single block\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_ptrs = row_start_ptr + col_offsets\n",
    "    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n",
    "    # Substract maximum for numerical stability\n",
    "    row_minus_max = row - tl.max(row, axis=0)\n",
    "    # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    # Write back output to DRAM\n",
    "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    output_ptrs = output_row_start_ptr + col_offsets\n",
    "    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "    # The block size is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n",
    "    # f the input matrix\n",
    "    softmax_kernel[(n_rows,)](\n",
    "        x,\n",
    "        y,\n",
    "        x.stride(0),\n",
    "        y.stride(0),\n",
    "        n_cols,\n",
    "        num_warps=num_warps,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# x = torch.randn(1823, 781, device='cuda')\n",
    "# y_triton = torch_softmax(x)\n",
    "# y_torch = torch.softmax(x, axis=1)\n",
    "# assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_dump_binary_path = \"./triton-binary.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_pow2 = lambda x: int(math.pow(2, math.ceil(math.log(x, 2))))\n",
    "\n",
    "\n",
    "def jt_softmax(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    out_shape = jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype)\n",
    "    block_size = next_pow2(x.shape[1])\n",
    "    strides = jt.strides_from_shape(x.shape)\n",
    "    return jt.triton_call(\n",
    "        x,\n",
    "        kernel=softmax_kernel,\n",
    "        out_shape=out_shape,\n",
    "        input_row_stride=strides[0],\n",
    "        output_row_stride=strides[0],\n",
    "        n_cols=x.shape[1],\n",
    "        grid=x.shape[0],\n",
    "        BLOCK_SIZE=block_size,\n",
    "        dump_binary_path=triton_dump_binary_path,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(rng, (1823, 781))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_jt = jt_softmax(x)\n",
    "y_jax = jax.nn.softmax(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(7.450581e-09, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_jt - y_jax).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_dump = pickle.load(open(triton_dump_binary_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ttir', 'ttgir', 'llir', 'ptx', 'cubin'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_dump[\"asm\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module attributes {\"triton_gpu.num-warps\" = 4 : i32, triton_gpu.shared = 512 : i32} {\n",
      "  llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n",
      "  llvm.func @softmax_kernel_0d1d(%arg0: !llvm.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1: !llvm.ptr<f32, 1> {tt.divisibility = 16 : i32}) attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = 128 : i32, sym_visibility = \"public\"} {\n",
      "    %0 = nvvm.read.ptx.sreg.tid.x : i32\n",
      "    %1 = llvm.mlir.constant(32 : i32) : i32\n",
      "    %2 = llvm.urem %0, %1  : i32\n",
      "    %3 = llvm.udiv %0, %1  : i32\n",
      "    %4 = llvm.urem %3, %1  : i32\n",
      "    %5 = llvm.mlir.constant(1024 : i32) : i32\n",
      "    %6 = llvm.urem %2, %5  : i32\n",
      "    %7 = llvm.mlir.constant(1 : i32) : i32\n",
      "    %8 = llvm.mul %4, %1  : i32\n",
      "    %9 = llvm.add %6, %8  : i32\n",
      "    %10 = llvm.mul %9, %7  : i32\n",
      "    %11 = llvm.mlir.constant(0 : i32) : i32\n",
      "    %12 = llvm.add %10, %11  : i32\n",
      "    %13 = llvm.mlir.constant(128 : i32) : i32\n",
      "    %14 = llvm.add %10, %13  : i32\n",
      "    %15 = llvm.mlir.constant(256 : i32) : i32\n",
      "    %16 = llvm.add %10, %15  : i32\n",
      "    %17 = llvm.mlir.constant(384 : i32) : i32\n",
      "    %18 = llvm.add %10, %17  : i32\n",
      "    %19 = llvm.mlir.constant(512 : i32) : i32\n",
      "    %20 = llvm.add %10, %19  : i32\n",
      "    %21 = llvm.mlir.constant(640 : i32) : i32\n",
      "    %22 = llvm.add %10, %21  : i32\n",
      "    %23 = llvm.mlir.constant(768 : i32) : i32\n",
      "    %24 = llvm.add %10, %23  : i32\n",
      "    %25 = llvm.mlir.constant(896 : i32) : i32\n",
      "    %26 = llvm.add %10, %25  : i32\n",
      "    %27 = llvm.mlir.addressof @global_smem : !llvm.ptr<array<0 x i8>, 3>\n",
      "    %28 = llvm.bitcast %27 : !llvm.ptr<array<0 x i8>, 3> to !llvm.ptr<i8, 3>\n",
      "    %29 = llvm.mlir.constant(781 : i32) : i32\n",
      "    %30 = llvm.mlir.constant(0xFF800000 : f32) : f32\n",
      "    %31 = nvvm.read.ptx.sreg.ctaid.x : i32\n",
      "    %32 = llvm.mul %31, %29  : i32\n",
      "    %33 = llvm.getelementptr %arg0[%32] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %34 = llvm.mlir.constant(0 : index) : i32\n",
      "    %35 = llvm.add %12, %34  : i32\n",
      "    %36 = llvm.add %14, %34  : i32\n",
      "    %37 = llvm.add %16, %34  : i32\n",
      "    %38 = llvm.add %18, %34  : i32\n",
      "    %39 = llvm.add %20, %34  : i32\n",
      "    %40 = llvm.add %22, %34  : i32\n",
      "    %41 = llvm.add %24, %34  : i32\n",
      "    %42 = llvm.add %26, %34  : i32\n",
      "    %43 = llvm.getelementptr %33[%35] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %44 = llvm.getelementptr %33[%36] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %45 = llvm.getelementptr %33[%37] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %46 = llvm.getelementptr %33[%38] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %47 = llvm.getelementptr %33[%39] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %48 = llvm.getelementptr %33[%40] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %49 = llvm.getelementptr %33[%41] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %50 = llvm.getelementptr %33[%42] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %51 = llvm.icmp \"slt\" %35, %29 : i32\n",
      "    %52 = llvm.icmp \"slt\" %36, %29 : i32\n",
      "    %53 = llvm.icmp \"slt\" %37, %29 : i32\n",
      "    %54 = llvm.icmp \"slt\" %38, %29 : i32\n",
      "    %55 = llvm.icmp \"slt\" %39, %29 : i32\n",
      "    %56 = llvm.icmp \"slt\" %40, %29 : i32\n",
      "    %57 = llvm.icmp \"slt\" %41, %29 : i32\n",
      "    %58 = llvm.icmp \"slt\" %42, %29 : i32\n",
      "    %59 = llvm.mlir.undef : vector<1xf32>\n",
      "    %60 = llvm.insertelement %30, %59[%34 : i32] : vector<1xf32>\n",
      "    %61 = llvm.bitcast %60 : vector<1xf32> to i32\n",
      "    %62 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %43, %51, %61, %51 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %63 = llvm.bitcast %62 : i32 to vector<1xf32>\n",
      "    %64 = llvm.extractelement %63[%34 : i32] : vector<1xf32>\n",
      "    %65 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %44, %52, %61, %52 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %66 = llvm.bitcast %65 : i32 to vector<1xf32>\n",
      "    %67 = llvm.extractelement %66[%34 : i32] : vector<1xf32>\n",
      "    %68 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %45, %53, %61, %53 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %69 = llvm.bitcast %68 : i32 to vector<1xf32>\n",
      "    %70 = llvm.extractelement %69[%34 : i32] : vector<1xf32>\n",
      "    %71 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %46, %54, %61, %54 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %72 = llvm.bitcast %71 : i32 to vector<1xf32>\n",
      "    %73 = llvm.extractelement %72[%34 : i32] : vector<1xf32>\n",
      "    %74 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %47, %55, %61, %55 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %75 = llvm.bitcast %74 : i32 to vector<1xf32>\n",
      "    %76 = llvm.extractelement %75[%34 : i32] : vector<1xf32>\n",
      "    %77 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %48, %56, %61, %56 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %78 = llvm.bitcast %77 : i32 to vector<1xf32>\n",
      "    %79 = llvm.extractelement %78[%34 : i32] : vector<1xf32>\n",
      "    %80 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %49, %57, %61, %57 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %81 = llvm.bitcast %80 : i32 to vector<1xf32>\n",
      "    %82 = llvm.extractelement %81[%34 : i32] : vector<1xf32>\n",
      "    %83 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 ld.global.b32 { $0 }, [ $1 + 0 ];\\0A\\09@!$4 mov.u32 $0, $3;\", \"=r,l,b,r,b\" %50, %58, %61, %58 : (!llvm.ptr<f32, 1>, i1, i32, i1) -> i32\n",
      "    %84 = llvm.bitcast %83 : i32 to vector<1xf32>\n",
      "    %85 = llvm.extractelement %84[%34 : i32] : vector<1xf32>\n",
      "    %86 = llvm.bitcast %28 : !llvm.ptr<i8, 3> to !llvm.ptr<f32, 3>\n",
      "    %87 = \"llvm.intr.maxnum\"(%64, %67) : (f32, f32) -> f32\n",
      "    %88 = \"llvm.intr.maxnum\"(%87, %70) : (f32, f32) -> f32\n",
      "    %89 = \"llvm.intr.maxnum\"(%88, %73) : (f32, f32) -> f32\n",
      "    %90 = \"llvm.intr.maxnum\"(%89, %76) : (f32, f32) -> f32\n",
      "    %91 = \"llvm.intr.maxnum\"(%90, %79) : (f32, f32) -> f32\n",
      "    %92 = \"llvm.intr.maxnum\"(%91, %82) : (f32, f32) -> f32\n",
      "    %93 = \"llvm.intr.maxnum\"(%92, %85) : (f32, f32) -> f32\n",
      "    %94 = llvm.icmp \"eq\" %2, %11 : i32\n",
      "    %95 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x10, 0x1f, 0xffffffff;\", \"=r,r\" %93 : (f32) -> f32\n",
      "    %96 = \"llvm.intr.maxnum\"(%93, %95) : (f32, f32) -> f32\n",
      "    %97 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x8, 0x1f, 0xffffffff;\", \"=r,r\" %96 : (f32) -> f32\n",
      "    %98 = \"llvm.intr.maxnum\"(%96, %97) : (f32, f32) -> f32\n",
      "    %99 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x4, 0x1f, 0xffffffff;\", \"=r,r\" %98 : (f32) -> f32\n",
      "    %100 = \"llvm.intr.maxnum\"(%98, %99) : (f32, f32) -> f32\n",
      "    %101 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x2, 0x1f, 0xffffffff;\", \"=r,r\" %100 : (f32) -> f32\n",
      "    %102 = \"llvm.intr.maxnum\"(%100, %101) : (f32, f32) -> f32\n",
      "    %103 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x1, 0x1f, 0xffffffff;\", \"=r,r\" %102 : (f32) -> f32\n",
      "    %104 = \"llvm.intr.maxnum\"(%102, %103) : (f32, f32) -> f32\n",
      "    %105 = llvm.getelementptr %86[%3] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>\n",
      "    %106 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.shared.b32 [ $0 + 0 ], $1;\", \"r,r,b\" %105, %104, %94 : (!llvm.ptr<f32, 3>, f32, i1) -> !llvm.void\n",
      "    nvvm.barrier0\n",
      "    %107 = llvm.getelementptr %86[%0] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>\n",
      "    %108 = llvm.load %107 : !llvm.ptr<f32, 3>\n",
      "    %109 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x2, 0x1f, 0xffffffff;\", \"=r,r\" %108 : (f32) -> f32\n",
      "    %110 = \"llvm.intr.maxnum\"(%108, %109) : (f32, f32) -> f32\n",
      "    %111 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x1, 0x1f, 0xffffffff;\", \"=r,r\" %110 : (f32) -> f32\n",
      "    %112 = \"llvm.intr.maxnum\"(%110, %111) : (f32, f32) -> f32\n",
      "    %113 = llvm.mlir.constant(4 : i32) : i32\n",
      "    %114 = llvm.icmp \"slt\" %0, %113 : i32\n",
      "    %115 = llvm.urem %2, %113  : i32\n",
      "    %116 = llvm.icmp \"eq\" %115, %11 : i32\n",
      "    %117 = llvm.and %114, %116  : i1\n",
      "    %118 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.shared.b32 [ $0 + 0 ], $1;\", \"r,r,b\" %107, %112, %117 : (!llvm.ptr<f32, 3>, f32, i1) -> !llvm.void\n",
      "    nvvm.barrier0\n",
      "    %119 = llvm.load %86 : !llvm.ptr<f32, 3>\n",
      "    %120 = llvm.fsub %64, %119  : f32\n",
      "    %121 = llvm.fsub %67, %119  : f32\n",
      "    %122 = llvm.fsub %70, %119  : f32\n",
      "    %123 = llvm.fsub %73, %119  : f32\n",
      "    %124 = llvm.fsub %76, %119  : f32\n",
      "    %125 = llvm.fsub %79, %119  : f32\n",
      "    %126 = llvm.fsub %82, %119  : f32\n",
      "    %127 = llvm.fsub %85, %119  : f32\n",
      "    %128 = llvm.mlir.constant(1.44269502 : f32) : f32\n",
      "    %129 = llvm.fmul %120, %128  : f32\n",
      "    %130 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %129 : (f32) -> f32\n",
      "    %131 = llvm.fmul %121, %128  : f32\n",
      "    %132 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %131 : (f32) -> f32\n",
      "    %133 = llvm.fmul %122, %128  : f32\n",
      "    %134 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %133 : (f32) -> f32\n",
      "    %135 = llvm.fmul %123, %128  : f32\n",
      "    %136 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %135 : (f32) -> f32\n",
      "    %137 = llvm.fmul %124, %128  : f32\n",
      "    %138 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %137 : (f32) -> f32\n",
      "    %139 = llvm.fmul %125, %128  : f32\n",
      "    %140 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %139 : (f32) -> f32\n",
      "    %141 = llvm.fmul %126, %128  : f32\n",
      "    %142 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %141 : (f32) -> f32\n",
      "    %143 = llvm.fmul %127, %128  : f32\n",
      "    %144 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"ex2.approx.f32 $0, $1;\", \"=f,f\" %143 : (f32) -> f32\n",
      "    nvvm.barrier0\n",
      "    %145 = llvm.fadd %130, %132  : f32\n",
      "    %146 = llvm.fadd %145, %134  : f32\n",
      "    %147 = llvm.fadd %146, %136  : f32\n",
      "    %148 = llvm.fadd %147, %138  : f32\n",
      "    %149 = llvm.fadd %148, %140  : f32\n",
      "    %150 = llvm.fadd %149, %142  : f32\n",
      "    %151 = llvm.fadd %150, %144  : f32\n",
      "    %152 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x10, 0x1f, 0xffffffff;\", \"=r,r\" %151 : (f32) -> f32\n",
      "    %153 = llvm.fadd %151, %152  : f32\n",
      "    %154 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x8, 0x1f, 0xffffffff;\", \"=r,r\" %153 : (f32) -> f32\n",
      "    %155 = llvm.fadd %153, %154  : f32\n",
      "    %156 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x4, 0x1f, 0xffffffff;\", \"=r,r\" %155 : (f32) -> f32\n",
      "    %157 = llvm.fadd %155, %156  : f32\n",
      "    %158 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x2, 0x1f, 0xffffffff;\", \"=r,r\" %157 : (f32) -> f32\n",
      "    %159 = llvm.fadd %157, %158  : f32\n",
      "    %160 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x1, 0x1f, 0xffffffff;\", \"=r,r\" %159 : (f32) -> f32\n",
      "    %161 = llvm.fadd %159, %160  : f32\n",
      "    %162 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.shared.b32 [ $0 + 0 ], $1;\", \"r,r,b\" %105, %161, %94 : (!llvm.ptr<f32, 3>, f32, i1) -> !llvm.void\n",
      "    nvvm.barrier0\n",
      "    %163 = llvm.load %107 : !llvm.ptr<f32, 3>\n",
      "    %164 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x2, 0x1f, 0xffffffff;\", \"=r,r\" %163 : (f32) -> f32\n",
      "    %165 = llvm.fadd %163, %164  : f32\n",
      "    %166 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"shfl.sync.bfly.b32 $0, $1, 0x1, 0x1f, 0xffffffff;\", \"=r,r\" %165 : (f32) -> f32\n",
      "    %167 = llvm.fadd %165, %166  : f32\n",
      "    %168 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.shared.b32 [ $0 + 0 ], $1;\", \"r,r,b\" %107, %167, %117 : (!llvm.ptr<f32, 3>, f32, i1) -> !llvm.void\n",
      "    nvvm.barrier0\n",
      "    %169 = llvm.load %86 : !llvm.ptr<f32, 3>\n",
      "    %170 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %130, %169 : (f32, f32) -> f32\n",
      "    %171 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %132, %169 : (f32, f32) -> f32\n",
      "    %172 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %134, %169 : (f32, f32) -> f32\n",
      "    %173 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %136, %169 : (f32, f32) -> f32\n",
      "    %174 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %138, %169 : (f32, f32) -> f32\n",
      "    %175 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %140, %169 : (f32, f32) -> f32\n",
      "    %176 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %142, %169 : (f32, f32) -> f32\n",
      "    %177 = llvm.inline_asm asm_dialect = att operand_attrs = [] \"div.full.f32 $0, $1, $2;\", \"=r,r,r\" %144, %169 : (f32, f32) -> f32\n",
      "    %178 = llvm.getelementptr %arg1[%32] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %179 = llvm.getelementptr %178[%35] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %180 = llvm.getelementptr %178[%36] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %181 = llvm.getelementptr %178[%37] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %182 = llvm.getelementptr %178[%38] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %183 = llvm.getelementptr %178[%39] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %184 = llvm.getelementptr %178[%40] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %185 = llvm.getelementptr %178[%41] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %186 = llvm.getelementptr %178[%42] : (!llvm.ptr<f32, 1>, i32) -> !llvm.ptr<f32, 1>\n",
      "    %187 = llvm.insertelement %170, %59[%11 : i32] : vector<1xf32>\n",
      "    %188 = llvm.bitcast %187 : vector<1xf32> to i32\n",
      "    %189 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %188, %179, %51 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    %190 = llvm.insertelement %171, %59[%11 : i32] : vector<1xf32>\n",
      "    %191 = llvm.bitcast %190 : vector<1xf32> to i32\n",
      "    %192 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %191, %180, %52 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    %193 = llvm.insertelement %172, %59[%11 : i32] : vector<1xf32>\n",
      "    %194 = llvm.bitcast %193 : vector<1xf32> to i32\n",
      "    %195 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %194, %181, %53 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    %196 = llvm.insertelement %173, %59[%11 : i32] : vector<1xf32>\n",
      "    %197 = llvm.bitcast %196 : vector<1xf32> to i32\n",
      "    %198 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %197, %182, %54 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    %199 = llvm.insertelement %174, %59[%11 : i32] : vector<1xf32>\n",
      "    %200 = llvm.bitcast %199 : vector<1xf32> to i32\n",
      "    %201 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %200, %183, %55 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    %202 = llvm.insertelement %175, %59[%11 : i32] : vector<1xf32>\n",
      "    %203 = llvm.bitcast %202 : vector<1xf32> to i32\n",
      "    %204 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %203, %184, %56 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    %205 = llvm.insertelement %176, %59[%11 : i32] : vector<1xf32>\n",
      "    %206 = llvm.bitcast %205 : vector<1xf32> to i32\n",
      "    %207 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %206, %185, %57 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    %208 = llvm.insertelement %177, %59[%11 : i32] : vector<1xf32>\n",
      "    %209 = llvm.bitcast %208 : vector<1xf32> to i32\n",
      "    %210 = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$2 st.global.b32 [ $1 + 0 ], { $0 };\", \"r,l,b\" %209, %186, %58 : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n",
      "    llvm.return\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(triton_dump[\"asm\"][\"ttir\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHCSA using the Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHCSATritonSoftmax(nn.Module):\n",
    "\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        T, C = x.shape  # sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        # [T, n_feat] @ [n_feat, T] -> [T, T].\n",
    "        att = (q @ k.T) * (1.0 / jnp.sqrt(self.n_feat))\n",
    "        causal_mask = jnp.tril(jnp.ones((T, T))).astype(bool)\n",
    "        att = jnp.where(~causal_mask, -jnp.inf, att)\n",
    "        att = jt_softmax(att)\n",
    "        # att = jax.nn.softmax(att, axis=-1)\n",
    "\n",
    "        y = att @ v  # [T, T] @ [T, n_feat] -> [T, n_feat]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for('gpt2')\n",
    "x = jax.random.normal(rng, (config.block_size, config.n_embd))\n",
    "n_feat = config.n_embd // config.n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = SingleHeadCausalSelfAttention(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty, _ = SHCSATritonSoftmax(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.7881393e-07, dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - ty).max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padded Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def padded_softmax_kernel(\n",
    "    att_ptr,\n",
    "    p_ptr,\n",
    "    output_ptr,\n",
    "    att_row_stride,\n",
    "    output_row_stride,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    n_padd = tl.load(p_ptr)\n",
    "\n",
    "    att_row_num = tl.program_id(0)\n",
    "    att_row_start_ptr = att_ptr + att_row_num * att_row_stride\n",
    "\n",
    "    att_col_idxs = tl.arange(0, BLOCK_SIZE)\n",
    "    att_ptrs = att_row_start_ptr + att_col_idxs\n",
    "\n",
    "    valid_mask = att_col_idxs < n_cols\n",
    "    causal_mask = att_col_idxs <= att_row_num\n",
    "    padd_mask = (att_col_idxs >= n_padd) & (att_row_num >= n_padd)\n",
    "    read_mask = valid_mask & causal_mask & padd_mask \n",
    "\n",
    "    att_row = tl.load(att_ptrs, mask=read_mask, other=-float(\"inf\"))\n",
    "\n",
    "    numerator = tl.exp(att_row - tl.max(att_row, axis=0))\n",
    "    sma_row = numerator / tl.sum(numerator, axis=0)\n",
    "\n",
    "    output_row_start_ptr = output_ptr + att_row_num * output_row_stride\n",
    "    output_ptrs = output_row_start_ptr + att_col_idxs\n",
    "\n",
    "    tl.store(output_ptrs, sma_row, mask=att_col_idxs < n_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_softmax(att, n_padd):\n",
    "\n",
    "    out_shape = jax.ShapeDtypeStruct(shape=att.shape, dtype=att.dtype)\n",
    "    block_size = next_pow2(att.shape[1])\n",
    "    strides = jt.strides_from_shape(att.shape)\n",
    "\n",
    "    return jt.triton_call(\n",
    "        att,\n",
    "        jnp.array(n_padd),\n",
    "        kernel=padded_softmax_kernel,\n",
    "        out_shape=out_shape,\n",
    "        att_row_stride=strides[0],\n",
    "        output_row_stride=strides[0],\n",
    "        n_cols=att.shape[1],\n",
    "        grid=att.shape[0],\n",
    "        BLOCK_SIZE=block_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jt.strides_from_shape((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHCSATritonPaddedSoftmax(nn.Module):\n",
    "\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, n_padd: int = 0):\n",
    "        T, C = x.shape  # sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        # [T, n_feat] @ [n_feat, T] -> [T, T].\n",
    "        att = (q @ k.T) * (1.0 / jnp.sqrt(self.n_feat))\n",
    "        att = padded_softmax(att, n_padd)\n",
    "\n",
    "        y = att @ v  # [T, T] @ [T, n_feat] -> [T, n_feat]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for('gpt2')\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (config.block_size, config.n_embd))\n",
    "n_feat = config.n_embd // config.n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = SingleHeadCausalSelfAttention(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty, _ = SHCSATritonPaddedSoftmax(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.7881393e-07, dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - ty).max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padded Softmax + v multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 55,  56,  57,  58,  59,  60,  61,  62,  63,  64],\n",
       "        [ 65,  66,  67,  68,  69,  70,  71,  72,  73,  74],\n",
       "        [ 75,  76,  77,  78,  79,  80,  81,  82,  83,  84],\n",
       "        [ 85,  86,  87,  88,  89,  90,  91,  92,  93,  94],\n",
       "        [ 95,  96,  97,  98,  99, 100, 101, 102, 103, 104],\n",
       "        [105, 106, 107, 108, 109, 110, 111, 112, 113, 114],\n",
       "        [115, 116, 117, 118, 119, 120, 121, 122, 123, 124],\n",
       "        [125, 126, 127, 128, 129, 130, 131, 132, 133, 134],\n",
       "        [135, 136, 137, 138, 139, 140, 141, 142, 143, 144],\n",
       "        [145, 146, 147, 148, 149, 150, 151, 152, 153, 154]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(0, 10) + 5)[:, None] * 10 + (torch.arange(0, 10) + 5)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 14, 15])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_row_stride = 64\n",
    "v_col_start = 3 * 4\n",
    "v_col_idxs = torch.arange(0, 4) + v_col_start\n",
    "seq_idxs = torch.arange(config.block_size)\n",
    "v_block_start_ptr = v_col_start\n",
    "v_col_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   24,    25,    26,    27],\n",
       "        [   88,    89,    90,    91],\n",
       "        [  152,   153,   154,   155],\n",
       "        ...,\n",
       "        [65368, 65369, 65370, 65371],\n",
       "        [65432, 65433, 65434, 65435],\n",
       "        [65496, 65497, 65498, 65499]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_block_start_ptr + (seq_idxs[:, None]*v_row_stride + v_col_idxs[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   12,    13,    14,    15],\n",
       "        [   76,    77,    78,    79],\n",
       "        [  140,   141,   142,   143],\n",
       "        ...,\n",
       "        [65356, 65357, 65358, 65359],\n",
       "        [65420, 65421, 65422, 65423],\n",
       "        [65484, 65485, 65486, 65487]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_col_start = 3 * 4\n",
    "v_block_start_ptr = 0 + v_col_start\n",
    "v_ptrs = v_block_start_ptr + (\n",
    "    seq_idxs[:, None] * v_row_stride + torch.arange(0, 4)[None, :]\n",
    ")\n",
    "v_ptrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   12,    13,    14,  ..., 65485, 65486, 65487])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ravel(v_ptrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[   12,    13,    14, ...,    61,    62,    63],\n",
       "       [   76,    77,    78, ...,   125,   126,   127],\n",
       "       [  140,   141,   142, ...,   189,   190,   191],\n",
       "       ...,\n",
       "       [65356, 65357, 65358, ..., 65405, 65406, 65407],\n",
       "       [65420, 65421, 65422, ..., 65469, 65470, 65471],\n",
       "       [65484, 65485, 65486, ..., 65533, 65534, 65535]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.arange(config.block_size * n_feat).reshape((config.block_size, n_feat))[:, 12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_padd = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_data_mask = (torch.arange(config.block_size) >= n_padd)[:, None] + torch.zeros(4, dtype=torch.uint8)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_data_mask[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_row = torch.randn(config.block_size)\n",
    "v_block = torch.randn(config.block_size, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2196, -2.0542,  1.1874, -1.0706],\n",
       "        [ 1.3401,  0.2011,  0.4685,  0.4783],\n",
       "        [ 0.9115, -0.1027, -0.6169,  0.6463],\n",
       "        ...,\n",
       "        [-0.0428,  0.8290,  1.2708, -1.0070],\n",
       "        [ 1.1806,  0.0735,  0.2074,  2.0656],\n",
       "        [-0.7963, -1.2081,  1.2733, -1.4956]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0366, -0.0366, -0.0366, -0.0366],\n",
       "        [-1.3061, -1.3061, -1.3061, -1.3061],\n",
       "        [-1.1283, -1.1283, -1.1283, -1.1283],\n",
       "        ...,\n",
       "        [ 0.8469,  0.8469,  0.8469,  0.8469],\n",
       "        [ 0.4464,  0.4464,  0.4464,  0.4464],\n",
       "        [ 1.2998,  1.2998,  1.2998,  1.2998]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sma_mat = torch.broadcast_to(sma_row[:, None], v_block.shape)\n",
    "sma_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 31.9814,  41.2079, -35.4196,   7.3629])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(sma_row, v_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 31.9814,  41.2079, -35.4195,   7.3629])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(sma_mat * v_block, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 31.9814,  41.2079, -35.4195,   7.3629])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(sma_row[:, None] * v_block, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def padded_softmax_v_kernel(\n",
    "    att_ptr,\n",
    "    v_ptr,\n",
    "    p_ptr,\n",
    "    output_ptr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    N_FEAT: tl.constexpr,\n",
    "    N_OCOLS: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel for computing the softmax of an attention matrix which may have padding\n",
    "    tokens, and then multiplying the result by a value matrix `v`.\n",
    "\n",
    "    Kernel cell with coordinates (i, j) computes out[i, j*N_OCOLS: (j+1)*N_OCOLS]. To\n",
    "    do this, it loads att[i, :] and v[:, j*N_OCOLS: (j+1)*N_OCOLS].\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    att_ptr: [SEQ_LEN, SEQ_LEN]\n",
    "    v_ptr: [SEQ_LEN, N_FEAT]\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    out: [SEQ_LEN, N_FEAT]\n",
    "        The output of self attention (`att @ v`).\n",
    "\n",
    "    Constants\n",
    "    ---------\n",
    "    SEQ_LEN: (1024 for GPT-2)\n",
    "    N_FEAT: (64 for GPT-2)\n",
    "    N_OCOLS: Number of elements of output matrix computed per kernel instance.\n",
    "\n",
    "    NOTE: Assumes all tensor sizes are powers of 2.\n",
    "    \"\"\"\n",
    "    n_padd = tl.load(p_ptr)\n",
    "\n",
    "    ## Load att[i, :] - with masking to avoid reading non-causal or padding tokens. ###\n",
    "    seq_idxs = tl.arange(0, SEQ_LEN)\n",
    "    att_row_num = tl.program_id(0)\n",
    "    att_row_start_ptr = att_ptr + att_row_num * SEQ_LEN\n",
    "    att_ptrs = att_row_start_ptr + seq_idxs\n",
    "\n",
    "    att_causal_mask = seq_idxs <= att_row_num  # 0 for non-causal tokens.\n",
    "    att_data_mask = (seq_idxs >= n_padd) & (\n",
    "        att_row_num >= n_padd\n",
    "    )  # 0 for padding tokens.\n",
    "    att_read_mask = att_causal_mask & att_data_mask\n",
    "\n",
    "    print(\"att_ptrs\", att_ptrs)\n",
    "    print(\"att_read_mask\", att_read_mask)\n",
    "    att_row = tl.load(att_ptrs, mask=att_read_mask, other=-float(\"inf\"))\n",
    "\n",
    "    ### Compute attention row softmax. ###\n",
    "\n",
    "    numerator = tl.exp(att_row - tl.max(att_row, axis=0))\n",
    "    sma_row = numerator / tl.sum(numerator, axis=0) # [SEQ_LEN,]\n",
    "\n",
    "    ### Load v[:, j*N_OCOLS: (j+1)*N_OCOLS] - with masking to avoid reading padding tokens. ###\n",
    "\n",
    "    v_col_start = tl.program_id(1) * N_OCOLS\n",
    "    v_block_start_ptr = v_ptr + v_col_start\n",
    "    v_ptrs = v_block_start_ptr + (\n",
    "        seq_idxs[:, None] * N_FEAT + tl.arange(0, N_OCOLS)[None, :]\n",
    "    )\n",
    "\n",
    "    # v[:n_padd, :] are values of padding tokens. The attention matrix already has zeros\n",
    "    # for elements corresponding to these. We use a mask to avoid unnecessary reads.\n",
    "    v_data_mask = (seq_idxs >= n_padd)[:, None] + tl.zeros((4,), dtype=tl.int1)[\n",
    "        None, :\n",
    "    ]\n",
    "\n",
    "    print(\"v_ptrs\", v_ptrs)\n",
    "    print(\"v_data_mask\", v_data_mask)\n",
    "    v_block = tl.load(v_ptrs, mask=v_data_mask, other = 0.0)\n",
    "\n",
    "    ### Compute output row-block. ###\n",
    "\n",
    "    # We want to compute sma_row @ v_block, but Trition doesn't support doing this\n",
    "    # directly, so we roll our own matrix-vector multiplication.\n",
    "    # [SEQ_LEN,] -> [SEQ_LEN, N_OCOLS] (the same column copied N_OCOLS times).\n",
    "    out = tl.sum(sma_row[:, None] * v_block, axis=0) # [N_OCOLS,]\n",
    "\n",
    "    ### Write output row-block. ###\n",
    "\n",
    "    output_start_ptr = output_ptr + att_row_num * N_FEAT + v_col_start\n",
    "    output_ptrs = output_start_ptr + tl.arange(0, N_OCOLS)\n",
    "\n",
    "    out_mask = tl.zeros((N_OCOLS,), dtype=tl.uint8) + att_row_num >= n_padd\n",
    "\n",
    "    # Don't bother writing outputs for padding tokens - just leave uninitialized.\n",
    "    tl.store(output_ptrs, out, mask=out_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_softmax_v(att, v, n_padd, n_ocols: int = 4):\n",
    "\n",
    "    out_shape = jax.ShapeDtypeStruct(shape=v.shape, dtype=v.dtype)\n",
    "    grid = (att.shape[0], v.shape[1] // n_ocols)\n",
    "    assert grid[1] * n_ocols == v.shape[1]\n",
    "\n",
    "    return jt.triton_call(\n",
    "        att,\n",
    "        v,\n",
    "        jnp.array(n_padd),\n",
    "        kernel=padded_softmax_v_kernel,\n",
    "        out_shape=out_shape,\n",
    "        grid=grid,\n",
    "        SEQ_LEN=att.shape[0],\n",
    "        N_FEAT=v.shape[1],\n",
    "        N_OCOLS=n_ocols,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHCSATritonPaddedSoftmaxV(nn.Module):\n",
    "\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, n_padd: int = 0):\n",
    "        T, C = x.shape  # sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        # [T, n_feat] @ [n_feat, T] -> [T, T].\n",
    "        att = (q @ k.T) * (1.0 / jnp.sqrt(self.n_feat))\n",
    "\n",
    "        y = padded_softmax_v(att, v, n_padd)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for('gpt2')\n",
    "rng = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(rng, (config.block_size, config.n_embd))\n",
    "n_feat = config.n_embd // config.n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = SingleHeadCausalSelfAttention(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_ptrs pointer<fp32>[constexpr[1024]]\n",
      "att_read_mask int1[constexpr[1024]]\n",
      "v_ptrs pointer<fp32>[constexpr[1024],constexpr[4]]\n",
      "v_data_mask int1[constexpr[1024],constexpr[4]]\n"
     ]
    }
   ],
   "source": [
    "ty, _ = SHCSATritonPaddedSoftmaxV(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_ptrs pointer<fp32>[constexpr[1024]]\n",
      "att_read_mask int1[constexpr[1024]]\n",
      "v_ptrs pointer<fp32>[constexpr[1024],constexpr[4]]\n",
      "v_data_mask int1[constexpr[1024],constexpr[4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:u32[2] b:f32[1024,768]. let\n",
       "    c:key<fry>[] = random_wrap[impl=fry] a\n",
       "    d:key<fry>[] = random_fold_in c 2998342421\n",
       "    e:u32[2] = random_unwrap d\n",
       "    f:f32[] = sqrt 0.0013020833721384406\n",
       "    g:f32[] = div f 0.879625678062439\n",
       "    h:key<fry>[] = random_wrap[impl=fry] e\n",
       "    i:f32[] = div -2.0 1.4142135381698608\n",
       "    j:f32[] = erf i\n",
       "    k:f32[] = div 2.0 1.4142135381698608\n",
       "    l:f32[] = erf k\n",
       "    m:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] j\n",
       "    n:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] l\n",
       "    o:u32[768,192] = random_bits[bit_width=32 shape=(768, 192)] h\n",
       "    p:u32[768,192] = shift_right_logical o 9\n",
       "    q:u32[768,192] = or p 1065353216\n",
       "    r:f32[768,192] = bitcast_convert_type[new_dtype=float32] q\n",
       "    s:f32[768,192] = sub r 1.0\n",
       "    t:f32[1,1] = sub n m\n",
       "    u:f32[768,192] = mul s t\n",
       "    v:f32[768,192] = add u m\n",
       "    w:f32[768,192] = max m v\n",
       "    x:f32[768,192] = erf_inv w\n",
       "    y:f32[768,192] = mul 1.4142135381698608 x\n",
       "    z:f32[] = stop_gradient -2.0\n",
       "    ba:f32[] = nextafter z inf\n",
       "    bb:f32[] = stop_gradient 2.0\n",
       "    bc:f32[] = nextafter bb -inf\n",
       "    bd:f32[768,192] = xla_call[\n",
       "      call_jaxpr={ lambda ; be:f32[768,192] bf:f32[] bg:f32[]. let\n",
       "          bh:f32[768,192] = max bf be\n",
       "          bi:f32[768,192] = min bg bh\n",
       "        in (bi,) }\n",
       "      name=clip\n",
       "    ] y ba bc\n",
       "    bj:f32[768,192] = mul bd g\n",
       "    bk:key<fry>[] = random_wrap[impl=fry] a\n",
       "    bl:key<fry>[] = random_fold_in bk 2299943050\n",
       "    _:u32[2] = random_unwrap bl\n",
       "    bm:f32[192] = broadcast_in_dim[broadcast_dimensions=() shape=(192,)] 0.0\n",
       "    bn:f32[1024,192] = dot_general[\n",
       "      dimension_numbers=(((1,), (0,)), ((), ()))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] b bj\n",
       "    bo:f32[1,192] = reshape[dimensions=None new_sizes=(1, 192)] bm\n",
       "    bp:f32[1024,192] = add bn bo\n",
       "    bq:f32[1024,64] = slice[\n",
       "      limit_indices=(1024, 64)\n",
       "      start_indices=(0, 0)\n",
       "      strides=None\n",
       "    ] bp\n",
       "    br:f32[1024,64] = slice[\n",
       "      limit_indices=(1024, 128)\n",
       "      start_indices=(0, 64)\n",
       "      strides=None\n",
       "    ] bp\n",
       "    bs:f32[1024,64] = slice[\n",
       "      limit_indices=(1024, 192)\n",
       "      start_indices=(0, 128)\n",
       "      strides=None\n",
       "    ] bp\n",
       "    bt:f32[64,1024] = transpose[permutation=(1, 0)] br\n",
       "    bu:f32[1024,1024] = dot_general[\n",
       "      dimension_numbers=(((1,), (0,)), ((), ()))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] bq bt\n",
       "    bv:f32[] = sqrt 64.0\n",
       "    bw:f32[] = div 1.0 bv\n",
       "    bx:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bw\n",
       "    by:f32[1024,1024] = mul bu bx\n",
       "    bz:f32[1024,64] = triton_kernel_call[\n",
       "      N_FEAT=64\n",
       "      N_OCOLS=4\n",
       "      SEQ_LEN=1024\n",
       "      asm=<jax_triton.triton_call.Asm object at 0x7f1bcf826530>\n",
       "      call_name=triton_kernel_call\n",
       "      dump_binary_path=None\n",
       "      grid=(1024, 16)\n",
       "      input_output_aliases=()\n",
       "      kernel_name=padded_softmax_v_kernel_0d1d2d3d\n",
       "      num_stages=2\n",
       "      num_warps=4\n",
       "      out_shapes=(ShapeDtypeStruct(shape=(1024, 64), dtype=float32),)\n",
       "      shared_mem=2064\n",
       "    ] by bs 0\n",
       "  in (bz, bm, bj) }"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(SHCSATritonPaddedSoftmaxV(n_feat).init_with_output)(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3841858e-07, dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - ty).max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padded Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = config.block_size\n",
    "N_FEAT = config.n_embd // config.n_head\n",
    "N_OCOLS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_padd = 5\n",
    "out_row_num = 7\n",
    "out_col_start = 3\n",
    "\n",
    "seq_idxs = torch.arange(0, SEQ_LEN)\n",
    "\n",
    "data_row_num_mask = out_row_num >= n_padd\n",
    "data_seq_mask = seq_idxs >= n_padd\n",
    "data_block_mask = torch.broadcast_to(data_seq_mask[:, None], (SEQ_LEN, N_OCOLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_block_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2147,  0.6030,  1.1701,  ..., -0.7375, -1.4493,  0.8846],\n",
       "        [-0.8668, -0.2372, -1.4559,  ...,  1.9100,  1.5226,  0.2802],\n",
       "        [-0.3820, -0.8772,  0.2808,  ..., -0.4392,  0.5333,  0.2044],\n",
       "        ...,\n",
       "        [-0.0188,  1.5368,  0.0277,  ..., -0.3134, -0.7780,  1.0785],\n",
       "        [ 0.5268,  0.5686,  1.2759,  ..., -0.7068, -2.0102, -0.6808],\n",
       "        [-1.5466,  0.2807,  0.2595,  ...,  0.3604, -0.5766,  1.0186]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_mat = torch.randn((SEQ_LEN, N_FEAT))\n",
    "k_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6096, -0.1009, -0.2483, -0.1537,  0.5777, -0.2142, -1.8165,  0.2700,\n",
       "         0.9725, -0.2513, -0.7535, -0.2081, -0.9549, -0.7695, -1.8244,  0.4653,\n",
       "         0.3364,  0.9533, -0.2115,  0.0030,  1.8236,  0.2614,  0.1525, -1.4166,\n",
       "        -0.2983, -0.3724, -0.0177,  1.3408, -1.1383, -1.1147,  0.6211, -0.4133,\n",
       "         0.0686,  0.6783, -0.0325,  1.6544,  0.2191, -0.2254, -0.2810, -0.0133,\n",
       "         0.0300, -1.1392,  1.6929, -0.5713,  0.9627, -1.2250, -0.7900,  1.1811,\n",
       "         1.1362,  0.0548,  0.5757, -0.3007,  1.0193, -1.1057, -0.0861, -1.3086,\n",
       "        -0.8898, -0.1812, -0.2406, -0.4369, -2.0844, -0.3743,  1.7599,  0.7849])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_row = torch.randn((N_FEAT,))\n",
    "q_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 64])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_row[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1309, -0.0609, -0.2906,  ...,  0.2760, -2.5506,  0.6944],\n",
       "        [-0.5284,  0.0239,  0.3615,  ..., -0.7148,  2.6797,  0.2199],\n",
       "        [-0.2328,  0.0885, -0.0697,  ...,  0.1644,  0.9386,  0.1604],\n",
       "        ...,\n",
       "        [-0.0115, -0.1551, -0.0069,  ...,  0.1173, -1.3692,  0.8466],\n",
       "        [ 0.3211, -0.0574, -0.3168,  ...,  0.2645, -3.5378, -0.5344],\n",
       "        [-0.9428, -0.0283, -0.0644,  ..., -0.1349, -1.0148,  0.7995]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_mat * q_row[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def padded_attention_kernel(\n",
    "    q_ptr,\n",
    "    k_ptr,\n",
    "    v_ptr,\n",
    "    p_ptr,\n",
    "    out_ptr,\n",
    "    SM_SCALE: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    N_FEAT: tl.constexpr,\n",
    "    N_OCOLS: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Triton kernel implementing single-headed attention with causal masking, where some\n",
    "    tokens may be padding.\n",
    "\n",
    "    Kernel cell with coordinates i, j computes out[i, j*N_OCOLS: (j+1)*N_OCOLS] by\n",
    "    multiplying att[i, :] by v[:, j*N_OCOLS: (j+1)*N_OCOLS].\n",
    "\n",
    "    We compute att[i, :] by multiplying q[i, :] by k[:, :]^T.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    q_ptr: [SEQ_LEN, N_FEAT]\n",
    "    k_ptr: [SEQ_LEN, N_FEAT]\n",
    "    v_ptr: [SEQ_LEN, N_FEAT]\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    out: [SEQ_LEN, N_FEAT]\n",
    "        The output of self attention (`att @ v`).\n",
    "\n",
    "    Constants\n",
    "    ---------\n",
    "    SEQ_LEN: (1024 for GPT-2)\n",
    "    N_FEAT: (64 for GPT-2)\n",
    "    N_OCOLS: Number of elements of output matrix computed per kernel instance.\n",
    "\n",
    "    NOTE: Assumes all tensor sizes are powers of 2.\n",
    "    \"\"\"\n",
    "    n_padd = tl.load(p_ptr)\n",
    "\n",
    "    out_row_num = tl.program_id(0)\n",
    "    out_col_start = tl.program_id(1) * N_OCOLS\n",
    "    # This cell computes out[out_row_num, out_col_start: out_col_start + N_OCOLS].\n",
    "\n",
    "    seq_idxs = tl.arange(0, SEQ_LEN)\n",
    "    feat_idxs = tl.arange(0, N_FEAT)\n",
    "    ocols_idxs = tl.arange(0, N_OCOLS)\n",
    "\n",
    "    # Shape (1,) mask. 0 if this instance is computing only padding elements of `out`.\n",
    "    data_row_num_mask = out_row_num >= n_padd\n",
    "    # Shape (SEQ_LEN,) mask. 0 for tokens of the sequence which are padding.\n",
    "    data_seq_mask = seq_idxs >= n_padd\n",
    "    # Shape (SEQ_LEN, N_OCOLS) mask. 0 for features of v corresponding to padding tokens.\n",
    "    data_v_block_mask = tl.broadcast_to(data_seq_mask[:, None], (SEQ_LEN, N_OCOLS))\n",
    "    # Shape (SEQ_LEN, N_FEAT) mask into k. 0 for features corresponding to padding tokens.\n",
    "    data_k_mat_mask = tl.broadcast_to(data_seq_mask[:, None], (SEQ_LEN, N_FEAT))\n",
    "    # Shape (SEQ_LEN,). 0 for non-causal elements of `att`.\n",
    "    causal_mask = seq_idxs <= out_row_num\n",
    "\n",
    "    ### Compute the softmax of att[out_row_num, :]. ###\n",
    "    # This requires loading one row of Q and all of K.\n",
    "\n",
    "    q_row_start_ptr = q_ptr + out_row_num * N_FEAT\n",
    "    q_ptrs = q_row_start_ptr + feat_idxs\n",
    "    q_row = tl.load(q_ptrs, mask=data_row_num_mask, other=0.0) # [N_FEAT,]\n",
    "\n",
    "    k_ptrs = k_ptr + (\n",
    "        seq_idxs[:, None] * N_FEAT + feat_idxs[None, :]\n",
    "    )\n",
    "    k_mat = tl.load(k_ptrs, mask=data_k_mat_mask, other=0.0) # [SEQ_LEN, N_FEAT]\n",
    "\n",
    "    # ([N_FEAT,] -> [1, N_FEAT]) * [SEQ_LEN, N_FEAT] -> [SEQ_LEN, N_FEAT].\n",
    "    att_row = tl.sum(q_row[None, :] * k_mat, axis=1) * SM_SCALE\n",
    "    # padding and non-causal elements currenly have value 0. We need to set them to -inf\n",
    "    # for the softmax.\n",
    "\n",
    "    print(\"q_row\", q_row)\n",
    "    print(\"k_mat\", k_mat)\n",
    "    print(\"att_row\", att_row)\n",
    "\n",
    "    causal_att_row = tl.where(causal_mask & data_seq_mask, att_row, float(\"-inf\"))\n",
    "    sm_numerator = tl.exp(causal_att_row - tl.max(causal_att_row, axis=0))\n",
    "    sm_att_row = sm_numerator / tl.sum(sm_numerator, axis=0) # [seq_len,]\n",
    "\n",
    "    ### Multiply att[out_row_num, :] by v[:, out_col_start: out_col_start + N_OCOLS]. ###\n",
    "\n",
    "    v_block_start_ptr = v_ptr + out_col_start\n",
    "    v_ptrs = v_block_start_ptr + (\n",
    "        seq_idxs[:, None] * N_FEAT + ocols_idxs[None, :]\n",
    "    )\n",
    "    v_block = tl.load(v_ptrs, mask=data_v_block_mask, other=0.0) # [SEQ_LEN, N_OCOLS]\n",
    "\n",
    "    out = tl.sum(sm_att_row[:, None] * v_block, axis=0) # [N_OCOLS,]\n",
    "\n",
    "    ### Write output row-block. ###\n",
    "\n",
    "    out_row_start_ptr = out_ptr + out_row_num * N_FEAT\n",
    "    out_ptrs = out_row_start_ptr + out_col_start + ocols_idxs\n",
    "\n",
    "    tl.store(out_ptrs, out, mask=data_row_num_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_attention(q, k, v, n_padd, n_ocols: int = 4):\n",
    "\n",
    "    out_shape = jax.ShapeDtypeStruct(shape=v.shape, dtype=v.dtype)\n",
    "    grid = (q.shape[0], q.shape[1] // n_ocols)\n",
    "    assert grid[1] * n_ocols == q.shape[1]\n",
    "\n",
    "    return jt.triton_call(\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        jnp.array(n_padd),\n",
    "        kernel=padded_attention_kernel,\n",
    "        out_shape=out_shape,\n",
    "        grid=grid,\n",
    "        SM_SCALE = 1.0 / k.shape[1]**0.5,\n",
    "        SEQ_LEN=q.shape[0],\n",
    "        N_FEAT=q.shape[1],\n",
    "        N_OCOLS=n_ocols,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHCSATriton(nn.Module):\n",
    "\n",
    "    n_feat: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, n_padd: int = 0):\n",
    "        T, C = x.shape  # sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # [T, C] @ [C, 3 * n_feat] -> [T, 3 * n_feat] -> 3 * [T, n_feat]\n",
    "        q, k, v = jnp.split(nn.Dense(features=3 * self.n_feat)(x), 3, axis=1)\n",
    "\n",
    "        y = padded_attention(q, k, v, n_padd)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_for('gpt2')\n",
    "rng = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(rng, (config.block_size, config.n_embd))\n",
    "n_feat = config.n_embd // config.n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = SingleHeadCausalSelfAttention(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_row fp32[constexpr[64]]\n",
      "k_mat fp32[constexpr[1024],constexpr[64]]\n",
      "att_row fp32[constexpr[1024]]\n"
     ]
    }
   ],
   "source": [
    "ty, _ = SHCSATriton(n_feat).init_with_output(rng, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3841858e-07, dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - ty).max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
