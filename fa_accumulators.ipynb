{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton.language as tl\n",
    "import triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_scale = 0.2\n",
    "Z = 64\n",
    "H = 48\n",
    "N_CTX = 1024\n",
    "D_HEAD = 64\n",
    "dtype=torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n",
    "k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2)\n",
    "v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n",
    "p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n",
    "for z in range(Z):\n",
    "    for h in range(H):\n",
    "        p[:, :, M == 0] = float(\"-inf\")\n",
    "p = torch.softmax(p.float(), dim=-1).half()\n",
    "# p = torch.exp(p)\n",
    "ref_out = torch.matmul(p, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_out = triton.ops.attention(q, k, v, sm_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ref_out - tri_out).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 48, 1024, 64])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def fa_kernel(\n",
    "    Q, K, V, sm_scale,\n",
    "    # L, M,\n",
    "    Out,\n",
    "    stride_qz, stride_qh, stride_qm, stride_qk,\n",
    "    stride_kz, stride_kh, stride_kn, stride_kk,\n",
    "    stride_vz, stride_vh, stride_vk, stride_vn,\n",
    "    stride_oz, stride_oh, stride_om, stride_on,\n",
    "    Z, H, N_CTX,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "):\n",
    "    start_m = tl.program_id(0)\n",
    "    off_hz = tl.program_id(1)\n",
    "    # initialize offsets\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    offs_d = tl.arange(0, BLOCK_DMODEL)\n",
    "    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n",
    "    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n",
    "    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n",
    "    # Initialize pointers to Q, K, V\n",
    "    q_ptrs = Q + off_q\n",
    "    k_ptrs = K + off_k\n",
    "    v_ptrs = V + off_v\n",
    "    # initialize pointer to m and l\n",
    "    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
    "    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
    "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n",
    "    # load q: it will stay in SRAM throughout\n",
    "    q = tl.load(q_ptrs)\n",
    "    # loop over k, v and update accumulator\n",
    "    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n",
    "        # -- compute qk ----\n",
    "        k = tl.load(k_ptrs + start_n * BLOCK_N * stride_kn)\n",
    "        # qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "        # qk += tl.dot(q, k)\n",
    "        qk = tl.dot(q, k)\n",
    "        qk *= sm_scale\n",
    "        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n",
    "        # compute new m\n",
    "        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n",
    "        # correct old l\n",
    "        # l_prev *= tl.exp(m_prev - m_curr)\n",
    "        # attention weights\n",
    "        p = tl.exp(qk - m_curr[:, None])\n",
    "        l_curr = tl.sum(p, 1) + l_prev\n",
    "        # rescale operands of matmuls\n",
    "        l_rcp = 1. / l_curr\n",
    "        p *= l_rcp\n",
    "        acc *= (l_prev * l_rcp)[:, None]\n",
    "        # update acc\n",
    "        p = p.to(tl.float16)\n",
    "        v = tl.load(v_ptrs + start_n * BLOCK_N * stride_vk)\n",
    "        acc += tl.dot(p, v)\n",
    "        # update m_i and l_i\n",
    "        # l_prev = l_curr\n",
    "        # m_prev = m_curr\n",
    "        # update pointers\n",
    "        # k_ptrs += BLOCK_N * stride_kn\n",
    "        # v_ptrs += BLOCK_N * stride_vk\n",
    "    # rematerialize offsets to save registers\n",
    "    # start_m = tl.program_id(0)\n",
    "    # offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    # write back l and m\n",
    "    # l_ptrs = L + off_hz * N_CTX + offs_m\n",
    "    # m_ptrs = M + off_hz * N_CTX + offs_m\n",
    "    # tl.store(l_ptrs, l_prev)\n",
    "    # tl.store(m_ptrs, m_prev)\n",
    "    # initialize pointers to output\n",
    "    offs_n = tl.arange(0, BLOCK_DMODEL)\n",
    "    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n",
    "    out_ptrs = Out + off_o\n",
    "    tl.store(out_ptrs, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_fa(q, k, v, sm_scale):\n",
    "    # only support for Ampere now\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    if capability[0] < 8:\n",
    "        raise RuntimeError(\"Flash attention currently only supported for compute capability < 80\")\n",
    "    BLOCK = 128\n",
    "    # shape constraints\n",
    "    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n",
    "    assert Lq == Lk and Lk == Lv\n",
    "    # assert Lk in {16, 32, 64, 128}\n",
    "    assert Lk in {64}  # TODO: fix other cases\n",
    "    o = torch.empty_like(q)\n",
    "    grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n",
    "    # L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
    "    # m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n",
    "    num_warps = 4 if Lk <= 64 else 8\n",
    "\n",
    "    fa_kernel[grid](\n",
    "        q, k, v, sm_scale,\n",
    "        # L, m,\n",
    "        o,\n",
    "        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n",
    "        q.shape[0], q.shape[1], q.shape[2],\n",
    "        BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n",
    "        BLOCK_DMODEL=Lk, num_warps=num_warps,\n",
    "        num_stages=2,\n",
    "    )\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_out = triton_fa(q, k, v, sm_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0293, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ref_out - fa_out).abs().max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton.testing import do_bench\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4307.456016540527"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_bench(partial(triton.ops.attention, q, k, v, sm_scale), warmup=100, rep=100)[0] * 1000 # us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3777.535915374756"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_bench(partial(triton_fa, q, k, v, sm_scale), warmup=100, rep=1000)[0] * 1000 # us"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
