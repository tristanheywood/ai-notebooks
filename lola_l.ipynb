{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lola L. (Layer nOrm + Linear + Activation + Linear)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `lola.ipynb`, implement batched float16 lola, with an additional linear and stack these layers to simulate the amount of computation performed by GPT.\n",
    "\n",
    "It looks like batching is required for Triton to beat jax/pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gpt-medium\n",
    "n_embd = 1024\n",
    "n_layer = 24\n",
    "\n",
    "n_btch = 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from nimblegpt import param_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(key, (n_btch, n_embd), dtype=jnp.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GELU(x):\n",
    "    return 0.5 * x * (1.0 +\n",
    "                      jnp.tanh(jnp.sqrt(2.0 / jnp.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "\n",
    "class FlaxLolal(nn.Module):\n",
    "    features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        *_, n_embd = x.shape\n",
    "\n",
    "        for _ in range(n_layer):\n",
    "            x = nn.LayerNorm(use_bias=False, use_scale=False)(x)\n",
    "            x = nn.Dense(self.features)(x)\n",
    "            x = GELU(x)\n",
    "            x = nn.Dense(n_embd)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_module = FlaxLolal(features=4 * n_embd)\n",
    "params = fl_module.init(key, x)\n",
    "params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float16), params)\n",
    "\n",
    "fl_apply = jax.jit(fl_module.apply)\n",
    "fy = fl_apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.77 ms ± 205 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "fl_apply(params, x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_shapes(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkernels = [\n",
    "    torch.tensor(np.array(params[\"params\"][f\"Dense_{i}\"][\"kernel\"]),\n",
    "                 device=\"cuda\") for i in range(2 * n_layer)\n",
    "]\n",
    "tweights = [tk.T for tk in tkernels]\n",
    "tbias = [\n",
    "    torch.tensor(np.array(params[\"params\"][f\"Dense_{i}\"][\"bias\"]),\n",
    "                 device=\"cuda\") for i in range(2 * n_layer)\n",
    "]\n",
    "\n",
    "tkernel1s = tkernels[0::2]\n",
    "tkernel2s = tkernels[1::2]\n",
    "tweight1s = tweights[0::2]\n",
    "tweight2s = tweights[1::2]\n",
    "tbias1s = tbias[0::2]\n",
    "tbias2s = tbias[1::2]\n",
    "\n",
    "tx = torch.tensor(np.array(x), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_lolal(x, weight1s, bias1s, weight2s, bias2s):\n",
    "\n",
    "    for w1, b1, w2, b2 in zip(weight1s, bias1s, weight2s, bias2s):\n",
    "        x = F.layer_norm(x, (n_embd, ))\n",
    "        x = F.linear(x, w1, b1)\n",
    "        x = F.gelu(x)\n",
    "        x = F.linear(x, w2, b2)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty = torch_lolal(tx, tweight1s, tbias1s, tweight2s, tbias2s)\n",
    "ty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01904, dtype=float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fy - ty.cpu().numpy()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 ms ± 126 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "torch_lolal(tx, tweight1s, tbias1s, tweight2s, tbias2s)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax Incremental Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Batched Multi-row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "sqrt2pi = math.sqrt(2.0 / math.pi)\n",
    "\n",
    "@triton.jit\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh activation function\"\"\"\n",
    "    return tl.libdevice.tanh(x)\n",
    "\n",
    "@triton.jit\n",
    "def fast_gelu(x):\n",
    "    \"\"\"Fast approximation of the gelu function. May slightly decrease accuracy.\"\"\"\n",
    "    return 0.5 * x * (1 + tanh(sqrt2pi * (x + 0.044715 * x * x * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dtype = tl.float16\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def lola_kernel(x_ptr, W_ptr, b_ptr, out_ptr, N_OCOLS: tl.constexpr,\n",
    "                N_BROWS: tl.constexpr, BLOCK_ROWS: tl.constexpr,\n",
    "                N_FEAT_IN: tl.constexpr, N_FEAT_OUT: tl.constexpr):\n",
    "    \"\"\"\n",
    "    Triton kernel implementing fused Layer nOrm, Linear and Activation.\n",
    "\n",
    "    Kernel cell (i, j) computes \n",
    "    out[i * N_BROWS: (i+1) * N_BROWS, j * N_OCOLS: (j+1) * N_OCOLS], by iterating over\n",
    "    `BLOCK_ROWS`-sized blocks of the inputs.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_ptr: [BATCH_SIZE, N_FEAT_IN,] - current token embedding.\n",
    "    W_ptr: [N_FEAT_IN, N_FEAT_OUT] - linear layer weights.\n",
    "    b_ptr: [N_FEAT_OUT,] - linear layer bias.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    out_ptr: [N_FEAT_OUT,] - output of the fused layer.\n",
    "    \"\"\"\n",
    "    x_brows_start = tl.program_id(0) * N_BROWS\n",
    "    # This instance will process x[b_rows_start:b_rows_start + N_BROWS, :]\n",
    "    x_brows_idxs = tl.arange(0, N_BROWS) + x_brows_start\n",
    "\n",
    "    ocols_start = tl.program_id(1) * N_OCOLS\n",
    "    col_idxs = tl.arange(0, N_OCOLS) + ocols_start\n",
    "\n",
    "    w_dot_x_acc = tl.zeros((N_BROWS, N_OCOLS), dtype=acc_dtype)\n",
    "    w_sum_acc = tl.zeros((N_OCOLS, ), dtype=acc_dtype)\n",
    "    x_sum_acc = tl.zeros((N_BROWS, ), dtype=acc_dtype)\n",
    "    x_sq_sum_acc = tl.zeros((N_BROWS, ), dtype=acc_dtype)\n",
    "\n",
    "    n_blocks = tl.cdiv(N_FEAT_IN, BLOCK_ROWS)\n",
    "    for block_i in range(0, n_blocks):\n",
    "\n",
    "        block_row_idxs = tl.arange(0, BLOCK_ROWS) + block_i * BLOCK_ROWS\n",
    "\n",
    "        # Load the current block of the input.\n",
    "        x_block_idxs = x_brows_idxs[:,\n",
    "                                    None] * N_FEAT_IN + block_row_idxs[None, :]\n",
    "        x_block = tl.load(x_ptr + x_block_idxs).to(\n",
    "            acc_dtype)  # [N_BROWS, BLOCK_ROWS]\n",
    "\n",
    "        W_block_idxs = block_row_idxs[:, None] * N_FEAT_OUT + col_idxs[None, :]\n",
    "        W_block = tl.load(W_ptr + W_block_idxs).to(\n",
    "            acc_dtype)  # [BLOCK_ROWS, N_OCOLS]\n",
    "\n",
    "        # Update the accumulators.\n",
    "        # [N_BROWS, BLOCK_ROWS] @ [BLOCK_ROWS, N_OCOLS] -> [N_BROWS, N_OCOLS]\n",
    "        w_dot_x_acc += tl.dot(x_block, W_block).to(acc_dtype)\n",
    "        # w_dot_x_acc += tl.sum(W_block * x_block[:, None], axis=0)\n",
    "        w_sum_acc += tl.sum(W_block, axis=0)\n",
    "        x_sum_acc += tl.sum(x_block, axis=1)\n",
    "        x_sq_sum_acc += tl.sum(x_block * x_block, axis=1)\n",
    "\n",
    "    bias = tl.load(b_ptr + col_idxs)\n",
    "    x_mean = x_sum_acc / N_FEAT_IN\n",
    "    x_sq_mean = x_sq_sum_acc / N_FEAT_IN\n",
    "\n",
    "    numer = w_dot_x_acc - x_mean[:, None] * w_sum_acc[None, :] + bias[None, :]\n",
    "    denom = tl.sqrt(tl.abs(x_sq_mean - x_mean * x_mean + 1e-5))\n",
    "    out = fast_gelu(numer / denom[:, None])\n",
    "\n",
    "    out_idxs = x_brows_idxs[:, None] * N_FEAT_OUT + col_idxs[None, :]\n",
    "    tl.store(out_ptr + out_idxs, out.to(tl.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_lola(\n",
    "    x,\n",
    "    W,\n",
    "    b,\n",
    "    N_OCOLS: int,\n",
    "    N_BROWS: int,\n",
    "    BLOCK_ROWS: int,\n",
    "    num_warps=4,\n",
    "    num_stages=1,\n",
    "):\n",
    "    assert N_BROWS >= 16 and BLOCK_ROWS >= 16 and N_OCOLS >= 16, \"Triton matrix multiplication requires matrix dimensions to be at least 16.\"\n",
    "\n",
    "    N_BATCH = x.shape[0]\n",
    "    N_FEAT_IN, N_FEAT_OUT = W.shape\n",
    "    grid = (\n",
    "        N_BATCH // N_BROWS,\n",
    "        N_FEAT_OUT // N_OCOLS,\n",
    "    )\n",
    "\n",
    "    # Allocate output buffer.\n",
    "    out = torch.zeros((N_BATCH, N_FEAT_OUT), dtype=x.dtype, device=\"cuda\")\n",
    "\n",
    "    # Launch the kernel.\n",
    "    lola_kernel[grid](x,\n",
    "                      W,\n",
    "                      b,\n",
    "                      out,\n",
    "                      N_OCOLS=N_OCOLS,\n",
    "                      N_BROWS=N_BROWS,\n",
    "                      BLOCK_ROWS=BLOCK_ROWS,\n",
    "                      N_FEAT_IN=N_FEAT_IN,\n",
    "                      N_FEAT_OUT=N_FEAT_OUT,\n",
    "                      num_warps=num_warps,\n",
    "                      num_stages=num_stages)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_lolal(x, kernel1s, bias1s, weight2s, bias2s, **kernel_kwargs):\n",
    "\n",
    "    for k1, b1, w2, b2 in zip(kernel1s, bias1s, weight2s, bias2s):\n",
    "        x = triton_lola(x, k1, b1, **kernel_kwargs)\n",
    "        x = F.linear(x, w2, b2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ky = triton_lolal(tx, tkernel1s, tbias1s, tweight2s, tbias2s, N_OCOLS=16, N_BROWS=16, BLOCK_ROWS=16, num_warps=4, num_stages=1)\n",
    "ky.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0426, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "(ty - ky).abs().max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Batched Multi-row Transposed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test transposing the weights matrix to see if it improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dtype = tl.float16\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def lola_trans_kernel(x_ptr, Wt_ptr, b_ptr, out_ptr, N_OCOLS: tl.constexpr,\n",
    "                N_BROWS: tl.constexpr, BLOCK_ROWS: tl.constexpr,\n",
    "                N_FEAT_IN: tl.constexpr, N_FEAT_OUT: tl.constexpr):\n",
    "    \"\"\"\n",
    "    Triton kernel implementing fused Layer nOrm, Linear and Activation.\n",
    "\n",
    "    Kernel cell (i, j) computes \n",
    "    out[i * N_BROWS: (i+1) * N_BROWS, j * N_OCOLS: (j+1) * N_OCOLS], by iterating over\n",
    "    `BLOCK_ROWS`-sized blocks of the inputs.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x_ptr: [BATCH_SIZE, N_FEAT_IN,] - current token embedding.\n",
    "    Wt_ptr: [N_FEAT_OUT, N_FEAT_IN] - linear layer weights (transposed).\n",
    "    b_ptr: [N_FEAT_OUT,] - linear layer bias.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    out_ptr: [N_FEAT_OUT,] - output of the fused layer.\n",
    "    \"\"\"\n",
    "    x_brows_start = tl.program_id(0) * N_BROWS\n",
    "    # This instance will process x[b_rows_start:b_rows_start + N_BROWS, :]\n",
    "    x_brows_idxs = tl.arange(0, N_BROWS) + x_brows_start\n",
    "\n",
    "    ocols_start = tl.program_id(1) * N_OCOLS\n",
    "    col_idxs = tl.arange(0, N_OCOLS) + ocols_start\n",
    "\n",
    "    w_dot_x_acc = tl.zeros((N_BROWS, N_OCOLS), dtype=acc_dtype)\n",
    "    w_sum_acc = tl.zeros((N_OCOLS, ), dtype=acc_dtype)\n",
    "    x_sum_acc = tl.zeros((N_BROWS, ), dtype=acc_dtype)\n",
    "    x_sq_sum_acc = tl.zeros((N_BROWS, ), dtype=acc_dtype)\n",
    "\n",
    "    n_blocks = tl.cdiv(N_FEAT_IN, BLOCK_ROWS)\n",
    "    for block_i in range(0, n_blocks):\n",
    "\n",
    "        block_row_idxs = tl.arange(0, BLOCK_ROWS) + block_i * BLOCK_ROWS\n",
    "\n",
    "        # Load the current block of the input.\n",
    "        x_block_idxs = x_brows_idxs[:,\n",
    "                                    None] * N_FEAT_IN + block_row_idxs[None, :]\n",
    "        x_block = tl.load(x_ptr + x_block_idxs).to(\n",
    "            acc_dtype)  # [N_BROWS, BLOCK_ROWS]\n",
    "\n",
    "        Wt_block_idxs = col_idxs[:, None] * N_FEAT_IN + block_row_idxs[None, :]\n",
    "        Wt_block = tl.load(Wt_ptr + Wt_block_idxs).to(acc_dtype)\n",
    "        # W_block = tl.trans(Wt_block)\n",
    "\n",
    "        print(x_block, Wt_block)\n",
    "        # Update the accumulators.\n",
    "        # [N_BROWS, BLOCK_ROWS] @ [BLOCK_ROWS, N_OCOLS] -> [N_BROWS, N_OCOLS]\n",
    "        w_dot_x_acc += tl.dot(x_block, tl.trans(Wt_block)).to(acc_dtype)\n",
    "        # w_dot_x_acc += tl.dot(Wt_block, tl.trans(x_block)).to(acc_dtype)\n",
    "        # w_dot_x_acc += tl.dot(x_block, W_block).to(acc_dtype)\n",
    "        # w_dot_x_acc += tl.dot(x_block, tl.trans(Wt_block)).to(acc_dtype)\n",
    "        # w_dot_x_acc += tl.sum(W_block * x_block[:, None], axis=0)\n",
    "        # w_sum_acc += tl.sum(W_block, axis=0)\n",
    "        w_sum_acc += tl.sum(Wt_block, axis=1)\n",
    "        x_sum_acc += tl.sum(x_block, axis=1)\n",
    "        x_sq_sum_acc += tl.sum(x_block * x_block, axis=1)\n",
    "\n",
    "    bias = tl.load(b_ptr + col_idxs)\n",
    "    x_mean = x_sum_acc / N_FEAT_IN\n",
    "    x_sq_mean = x_sq_sum_acc / N_FEAT_IN\n",
    "\n",
    "    numer = w_dot_x_acc - x_mean[:, None] * w_sum_acc[None, :] + bias[None, :]\n",
    "    denom = tl.sqrt(tl.abs(x_sq_mean - x_mean * x_mean + 1e-5))\n",
    "    out = fast_gelu(numer / denom[:, None])\n",
    "\n",
    "    out_idxs = x_brows_idxs[:, None] * N_FEAT_OUT + col_idxs[None, :]\n",
    "    tl.store(out_ptr + out_idxs, out.to(tl.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_lola_trans(\n",
    "    x,\n",
    "    W_trans,\n",
    "    b,\n",
    "    N_OCOLS: int,\n",
    "    N_BROWS: int,\n",
    "    BLOCK_ROWS: int,\n",
    "    num_warps=4,\n",
    "    num_stages=1,\n",
    "):\n",
    "    assert N_BROWS >= 16 and BLOCK_ROWS >= 16 and N_OCOLS >= 16, \"Triton matrix multiplication requires matrix dimensions to be at least 16.\"\n",
    "\n",
    "    N_BATCH = x.shape[0]\n",
    "    N_FEAT_OUT, N_FEAT_IN = W_trans.shape\n",
    "    grid = (\n",
    "        N_BATCH // N_BROWS,\n",
    "        N_FEAT_OUT // N_OCOLS,\n",
    "    )\n",
    "\n",
    "    # Allocate output buffer.\n",
    "    out = torch.zeros((N_BATCH, N_FEAT_OUT), dtype=x.dtype, device=\"cuda\")\n",
    "\n",
    "    # Launch the kernel.\n",
    "    lola_trans_kernel[grid](x,\n",
    "                      W_trans,\n",
    "                      b,\n",
    "                      out,\n",
    "                      N_OCOLS=N_OCOLS,\n",
    "                      N_BROWS=N_BROWS,\n",
    "                      BLOCK_ROWS=BLOCK_ROWS,\n",
    "                      N_FEAT_IN=N_FEAT_IN,\n",
    "                      N_FEAT_OUT=N_FEAT_OUT,\n",
    "                      num_warps=num_warps,\n",
    "                      num_stages=num_stages)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_lolal_trans(x, weight1s, bias1s, weight2s, bias2s, **kernel_kwargs):\n",
    "\n",
    "    for w1, b1, w2, b2 in zip(weight1s, bias1s, weight2s, bias2s):\n",
    "        x = triton_lola_trans(x, w1, b1, **kernel_kwargs)\n",
    "        x = F.linear(x, w2, b2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp16[constexpr[16],constexpr[16]] fp16[constexpr[16],constexpr[16]]\n",
      "fp16[constexpr[16],constexpr[16]] fp16[constexpr[16],constexpr[16]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tky = triton_lolal_trans(tx, tweight1s, tbias1s, tweight2s, tbias2s, N_OCOLS=16, N_BROWS=16, BLOCK_ROWS=16, num_warps=4, num_stages=1)\n",
    "tky.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7734, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ty - tky).abs().max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = tweight1s[0]\n",
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.zeros_like(w1)\n",
    "w1[:, 1] = 1\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0402,  0.2347, -0.0448,  ...,  0.0795,  0.4070, -0.1065],\n",
       "        [ 0.1570, -0.1503,  1.0068,  ...,  0.2034, -0.0872, -0.1332],\n",
       "        [ 0.5347, -0.0126,  0.0631,  ...,  0.7222,  0.5054,  1.6523],\n",
       "        ...,\n",
       "        [ 0.6846, -0.1473,  2.4023,  ...,  0.0865,  0.6001,  0.1892],\n",
       "        [ 0.0549, -0.1543,  0.9678,  ...,  1.3838,  1.2451,  0.4380],\n",
       "        [-0.1488,  1.0879,  0.0668,  ..., -0.1276,  0.5479, -0.1592]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kout = triton_lola_trans(tx, tweight1s[0], tbias1s[0], N_OCOLS=16, N_BROWS=32, BLOCK_ROWS=64,num_warps=4, num_stages=1)\n",
    "kout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kout = triton_lola_trans(tx, w1, tbias1s[0], N_OCOLS=16, N_BROWS=32, BLOCK_ROWS=64,num_warps=4, num_stages=1)\n",
    "kout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000, -0.0153, -0.0153, -0.0153, -0.0153,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0165,  0.0165,  0.0165,  0.0165,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0331,  0.0331,  0.0331,  0.0331,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0412,  0.0412,  0.0412,  0.0412,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0158, -0.0158, -0.0158, -0.0158,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0078, -0.0078, -0.0078, -0.0078,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0077,  0.0077,  0.0077,  0.0077,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0079,  0.0079,  0.0079,  0.0079,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0079, -0.0079, -0.0079, -0.0079,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0077,  0.0077,  0.0077,  0.0077,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0684,  0.0684,  0.0684,  0.0684,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0325,  0.0325,  0.0325,  0.0325,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0116,  0.0116,  0.0116,  0.0116,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0334,  0.0334,  0.0334,  0.0334,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0149, -0.0149, -0.0149, -0.0149,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0153, -0.0153, -0.0153, -0.0153,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0074, -0.0074, -0.0074, -0.0074,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0162,  0.0162,  0.0162,  0.0162,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0162,  0.0162,  0.0162,  0.0162,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0160,  0.0160,  0.0160,  0.0160,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0293, -0.0293, -0.0293, -0.0293,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kout[:30, 2:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039, dtype=torch.float16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kout[:, 0].cpu() - np.array(out[:, 0])).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1205., device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kout.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = params[\"params\"][\"Dense_0\"][\"kernel\"]\n",
    "b1 = params[\"params\"][\"Dense_0\"][\"bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_dot_x = jnp.dot(x, k1)\n",
    "w_sum = jnp.sum(k1, axis = 0)\n",
    "x_mean = jnp.mean(x, axis=1)\n",
    "x_sq_mean = jnp.mean(x * x, axis=1)\n",
    "\n",
    "numer = w_dot_x - jnp.expand_dims(x_mean, axis=1) * w_sum + b1\n",
    "denom = jnp.sqrt(jnp.abs(x_sq_mean - x_mean * x_mean + 1e-5))\n",
    "\n",
    "out = GELU(numer / jnp.expand_dims(denom, axis=1)) # [BATCH_SIZE, N_FEAT_OUT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.194  , -0.1282 , -0.1147 , ..., -0.1663 , -0.05746, -0.1678 ],\n",
       "       [-0.1307 ,  0.00922,  0.1714 , ..., -0.1691 , -0.1509 ,  0.1203 ],\n",
       "       [ 0.5483 ,  0.06   ,  0.02933, ...,  0.11194, -0.08325, -0.1069 ],\n",
       "       ...,\n",
       "       [ 1.316  , -0.0973 ,  0.3074 , ..., -0.0041 ,  1.623  ,  0.4402 ],\n",
       "       [ 1.132  ,  0.443  , -0.1521 , ..., -0.10846,  0.2368 , -0.0942 ],\n",
       "       [-0.1692 ,  0.181  ,  0.531  , ..., -0.1691 ,  0.595  , -0.1252 ]],      dtype=float16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.305  , -1.251  , -0.2788 , ..., -0.8896 , -1.838  , -0.904  ],\n",
       "       [-0.35   , -0.0188 ,  0.2534 , ..., -0.819  , -1.114  ,  0.2705 ],\n",
       "       [ 0.7075 ,  0.07965,  0.03482, ...,  0.1906 , -0.2156 , -1.359  ],\n",
       "       ...,\n",
       "       [ 1.442  , -0.2241 ,  0.477  , ..., -0.01009,  1.745  ,  0.58   ],\n",
       "       [ 1.325  ,  0.6284 , -1.131  , ..., -1.504  ,  0.3782 , -1.606  ],\n",
       "       [-0.815  ,  0.281  ,  0.6904 , ..., -0.6914 ,  0.7534 , -1.282  ]],      dtype=float16)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_dot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-1078., dtype=float16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_dot_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01758"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(out) - triton_lola(tx, tkernel1s[0], tbias1s[0], N_OCOLS=16, N_BROWS=16, BLOCK_ROWS=16, num_warps=4, num_stages=1).cpu().numpy()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.938"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(out) - kout.cpu().numpy()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kout = triton_lola_trans(tx, tkernel1s[0], tbias1s[0], N_OCOLS=16, N_BROWS=32, BLOCK_ROWS=64,num_warps=4, num_stages=1)\n",
    "kout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2438,  1.3438,  2.8867,  ..., -2.0781, -0.3979, -0.1449],\n",
       "        [-0.4087,  2.7031,  1.5830,  ..., -0.8726,  0.0990,  1.9658],\n",
       "        [ 0.2698,  1.9844, -1.7441,  ...,  1.7529,  2.0352, -1.2568],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.305  , -1.251  , -0.2788 , ..., -0.8896 , -1.838  , -0.904  ],\n",
       "       [-0.35   , -0.0188 ,  0.2534 , ..., -0.819  , -1.114  ,  0.2705 ],\n",
       "       [ 0.7075 ,  0.07965,  0.03482, ...,  0.1906 , -0.2156 , -1.359  ],\n",
       "       ...,\n",
       "       [ 1.442  , -0.2241 ,  0.477  , ..., -0.01009,  1.745  ,  0.58   ],\n",
       "       [ 1.325  ,  0.6284 , -1.131  , ..., -1.504  ,  0.3782 , -1.606  ],\n",
       "       [-0.815  ,  0.281  ,  0.6904 , ..., -0.6914 ,  0.7534 , -1.282  ]],      dtype=float16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_dot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton.testing import do_bench\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ocols=16, n_brows=32, block_rows=512: 29713.41 us\n",
      "n_ocols=16, n_brows=64, block_rows=256: 22185.47 us\n",
      "n_ocols=16, n_brows=64, block_rows=512: 470851.59 us\n",
      "n_ocols=16, n_brows=128, block_rows=128: 18547.71 us\n",
      "n_ocols=16, n_brows=128, block_rows=256: 41585.66 us\n",
      "n_ocols=16, n_brows=256, block_rows=64: 17542.14 us\n",
      "n_ocols=16, n_brows=256, block_rows=128: 25800.70 us\n",
      "n_ocols=16, n_brows=512, block_rows=32: 14923.26 us\n",
      "n_ocols=16, n_brows=512, block_rows=64: 18748.93 us\n",
      "n_ocols=32, n_brows=16, block_rows=512: 21115.90 us\n",
      "n_ocols=32, n_brows=32, block_rows=256: 16519.68 us\n",
      "n_ocols=32, n_brows=32, block_rows=512: 17985.54 us\n",
      "n_ocols=32, n_brows=64, block_rows=128: 12158.46 us\n",
      "n_ocols=32, n_brows=64, block_rows=256: 12904.45 us\n",
      "n_ocols=32, n_brows=128, block_rows=64: 11728.38 us\n",
      "n_ocols=32, n_brows=128, block_rows=128: 10908.67 us\n",
      "n_ocols=32, n_brows=256, block_rows=32: 12298.75 us\n",
      "n_ocols=32, n_brows=256, block_rows=64: 10683.39 us\n",
      "n_ocols=32, n_brows=512, block_rows=16: 15394.82 us\n",
      "n_ocols=32, n_brows=512, block_rows=32: 13635.58 us\n",
      "n_ocols=64, n_brows=16, block_rows=256: 11908.10 us\n",
      "n_ocols=64, n_brows=16, block_rows=512: 117773.32 us\n",
      "n_ocols=64, n_brows=32, block_rows=128: 9321.47 us\n",
      "n_ocols=64, n_brows=32, block_rows=256: 9783.30 us\n",
      "n_ocols=64, n_brows=64, block_rows=64: 8776.70 us\n",
      "n_ocols=64, n_brows=64, block_rows=128: 8097.79 us\n",
      "n_ocols=64, n_brows=128, block_rows=32: 9450.50 us\n",
      "n_ocols=64, n_brows=128, block_rows=64: 8032.26 us\n",
      "n_ocols=64, n_brows=256, block_rows=16: 19976.70 us\n",
      "n_ocols=64, n_brows=256, block_rows=32: 10370.05 us\n",
      "n_ocols=64, n_brows=512, block_rows=16: 15093.76 us\n",
      "n_ocols=128, n_brows=16, block_rows=128: 9476.10 us\n",
      "n_ocols=128, n_brows=16, block_rows=256: 9351.68 us\n",
      "n_ocols=128, n_brows=32, block_rows=64: 7811.07 us\n",
      "n_ocols=128, n_brows=32, block_rows=128: 7308.29 us\n",
      "n_ocols=128, n_brows=64, block_rows=32: 8510.46 us\n",
      "n_ocols=128, n_brows=64, block_rows=64: 5895.17 us\n",
      "n_ocols=128, n_brows=128, block_rows=16: 12238.85 us\n",
      "n_ocols=128, n_brows=128, block_rows=32: 10665.98 us\n",
      "n_ocols=128, n_brows=256, block_rows=16: 12085.76 us\n",
      "n_ocols=256, n_brows=16, block_rows=64: 8644.61 us\n",
      "n_ocols=256, n_brows=16, block_rows=128: 7937.02 us\n",
      "n_ocols=256, n_brows=32, block_rows=32: 7467.01 us\n",
      "n_ocols=256, n_brows=32, block_rows=64: 5742.59 us\n",
      "n_ocols=256, n_brows=64, block_rows=16: 11475.97 us\n",
      "n_ocols=256, n_brows=64, block_rows=32: 7534.59 us\n",
      "n_ocols=256, n_brows=128, block_rows=16: 12849.15 us\n",
      "n_ocols=512, n_brows=16, block_rows=32: 7826.43 us\n",
      "n_ocols=512, n_brows=16, block_rows=64: 7523.33 us\n",
      "n_ocols=512, n_brows=32, block_rows=16: 9160.70 us\n",
      "n_ocols=512, n_brows=32, block_rows=32: 7090.18 us\n",
      "n_ocols=512, n_brows=64, block_rows=16: 9338.88 us\n"
     ]
    }
   ],
   "source": [
    "func = partial(triton_lolal, tx, tkernel1s, tbias1s, tweight2s, tbias2s)\n",
    "\n",
    "for n_ocols in [16, 32, 64, 128, 256, 512]:\n",
    "    for n_brows in [16, 32, 64, 128, 256, 512]:\n",
    "        for block_rows in [16, 32, 64, 128, 256, 512]:\n",
    "            if n_ocols * n_brows * block_rows < 200_000:\n",
    "                continue\n",
    "            if n_ocols * n_brows * block_rows / 4 > 166_000:\n",
    "                continue\n",
    "            print(f\"{n_ocols=}, {n_brows=}, {block_rows=}\", end=\": \")\n",
    "            print(f\"{do_bench(partial(func, N_OCOLS=n_ocols, N_BROWS=n_brows, BLOCK_ROWS=block_rows), warmup = 100, rep = 100)[0]:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ocols=64, n_brows=16, block_rows=16: 71.27 ms\n",
      "n_ocols=64, n_brows=16, block_rows=32: 38.81 ms\n",
      "n_ocols=64, n_brows=16, block_rows=64: 21.98 ms\n",
      "n_ocols=64, n_brows=16, block_rows=128: 14.59 ms\n",
      "n_ocols=64, n_brows=16, block_rows=256: 11.91 ms\n",
      "n_ocols=64, n_brows=32, block_rows=16: 38.95 ms\n",
      "n_ocols=64, n_brows=32, block_rows=32: 20.96 ms\n",
      "n_ocols=64, n_brows=32, block_rows=64: 12.78 ms\n",
      "n_ocols=64, n_brows=32, block_rows=128: 9.32 ms\n",
      "n_ocols=64, n_brows=32, block_rows=256: 9.79 ms\n",
      "n_ocols=64, n_brows=64, block_rows=16: 21.61 ms\n",
      "n_ocols=64, n_brows=64, block_rows=32: 14.17 ms\n",
      "n_ocols=64, n_brows=64, block_rows=64: 8.78 ms\n",
      "n_ocols=64, n_brows=64, block_rows=128: 8.10 ms\n",
      "n_ocols=128, n_brows=16, block_rows=16: 31.89 ms\n",
      "n_ocols=128, n_brows=16, block_rows=32: 19.09 ms\n",
      "n_ocols=128, n_brows=16, block_rows=64: 11.65 ms\n",
      "n_ocols=128, n_brows=16, block_rows=128: 9.48 ms\n",
      "n_ocols=128, n_brows=16, block_rows=256: 9.35 ms\n",
      "n_ocols=128, n_brows=32, block_rows=16: 18.41 ms\n",
      "n_ocols=128, n_brows=32, block_rows=32: 12.23 ms\n",
      "n_ocols=128, n_brows=32, block_rows=64: 7.81 ms\n",
      "n_ocols=128, n_brows=32, block_rows=128: 7.31 ms\n",
      "n_ocols=128, n_brows=64, block_rows=16: 13.10 ms\n",
      "n_ocols=128, n_brows=64, block_rows=32: 8.47 ms\n",
      "n_ocols=128, n_brows=64, block_rows=64: 5.89 ms\n",
      "n_ocols=256, n_brows=16, block_rows=16: 15.31 ms\n",
      "n_ocols=256, n_brows=16, block_rows=32: 9.86 ms\n",
      "n_ocols=256, n_brows=16, block_rows=64: 8.65 ms\n",
      "n_ocols=256, n_brows=16, block_rows=128: 7.94 ms\n",
      "n_ocols=256, n_brows=32, block_rows=16: 11.42 ms\n",
      "n_ocols=256, n_brows=32, block_rows=32: 7.46 ms\n",
      "n_ocols=256, n_brows=32, block_rows=64: 5.76 ms\n",
      "n_ocols=256, n_brows=64, block_rows=16: 10.29 ms\n",
      "n_ocols=256, n_brows=64, block_rows=32: 7.51 ms\n",
      "n_ocols=512, n_brows=16, block_rows=16: 10.59 ms\n",
      "n_ocols=512, n_brows=16, block_rows=32: 7.78 ms\n",
      "n_ocols=512, n_brows=16, block_rows=64: 7.50 ms\n",
      "n_ocols=512, n_brows=32, block_rows=16: 9.15 ms\n",
      "n_ocols=512, n_brows=32, block_rows=32: 7.07 ms\n",
      "n_ocols=512, n_brows=64, block_rows=16: 9.32 ms\n"
     ]
    }
   ],
   "source": [
    "func = partial(triton_lolal, tx, tkernel1s, tbias1s, tweight2s, tbias2s)\n",
    "\n",
    "for n_ocols in [64, 128, 256, 512]:\n",
    "    for n_brows in [16, 32, 64]:\n",
    "        for block_rows in [16, 32, 64, 128, 256]:\n",
    "            if n_ocols * n_brows * block_rows / 4 > 166_000:\n",
    "                continue\n",
    "            print(f\"{n_ocols=}, {n_brows=}, {block_rows=}\", end=\": \")\n",
    "            print(f\"{do_bench(partial(func, N_OCOLS=n_ocols, N_BROWS=n_brows, BLOCK_ROWS=block_rows), warmup = 100, rep = 100)[0]:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
